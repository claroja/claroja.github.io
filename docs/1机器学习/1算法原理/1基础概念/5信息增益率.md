# 信息增益率


## 最佳实践

## 概念






在[信息增益](5信息增益.md)一文中, 若把“编号”也作为一个候选划分属性，则根据可计算出它的信息增益远大于其他候选划分属性。这很容易理解：“编号”将产生10个分支，每个分支结点仅包含一个样本，这些分支结点的纯度已达最大。然而这样的决策树显然不具有泛化能力，无法对新样本进行有效预测。

信息增益准则对可取值数目较多的属性有所偏好，为减少这种偏好可能带来的不利影响，著名的 C4.5 决策树算法不直接使用信息增益，而是使用增益率(gain ratio)来选择最优划分属性。

信息增益率引入了归一化项(normalizing term), 称为Intrinsic Information (II) .公式如下:

$$
II = - (\sum \frac{|D_j|}{|D|} * \log_2\frac{|D_j|}{|D|})
$$

其中:
1. $|D_j|$表示特征某个类别的样本数量, 注意不是标签
2. $|D|$表示特征整体的样本数量, 注意不是标签

$\text{II}$的含义是评估特征特征本身的熵. 特征的分类数越多, 它的熵就越高.


带入[信息增益](5信息增益.md)中的值, 

$$
\text{II} = - \left( \frac{35}{100} \cdot \log_2 \frac{35}{100} + \frac{65}{100} \cdot \log_2 \frac{65}{100} \right) \approx 1
$$


信息增益率的公式为:

$$
Gain Ratio = \frac{\text{Information Gain}}{\text{Intrinsic Information}}
$$

带入[信息增益](5信息增益.md)中的信息增益值:

$$
\text{Gain Ratio} = 0.1245/1 = 0.1245
$$



假如该特征$X$, 被分为了5类, 则$\text{II}$
$$
I = - \left( \frac{2}{10} \cdot \log_2 \frac{2}{10} + \frac{2}{10} \cdot \log_2 \frac{2}{10} + \frac{2}{10} \cdot \log_2 \frac{2}{10}+ \frac{2}{10} \cdot \log_2 \frac{2}{10}+ \frac{2}{10} \cdot \log_2 \frac{2}{10}\right) \approx 2.3219
$$

很明显$\text{II}$变大了, 因为该特征本身的熵就高. 作为信息增益率的分母, 也会导致信息增益率下降.

$$
\text{Gain Ratio} = 0.1245/2.3219 \approx 0.0536
$$







## 参考
1. https://xiaosheng.blog/2018/04/07/decision-tree
2. https://tungmphung.com/information-gain-gain-ratio-and-gini-index/