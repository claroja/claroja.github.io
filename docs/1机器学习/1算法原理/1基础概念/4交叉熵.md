交叉熵 "一词包含 "交叉 "和 "熵"，理解了 "熵 "的部分，才能理解 "交叉 "的部分。



## 熵公式回顾

对于离散变量, 概率分布的熵值如下：

$$
\text{Entropy} = - \sum_i P(i)log_2P(i)
$$

对于连续变量，可以用积分形式来写：

$$
\text{Entropy} = -\int P(x)logP(x)dx
$$

其中:
1. x 是连续变量
2. P(x) 是概率密度函数。


无论是离散变量还是连续变量，我们计算的都是负对数概率的期望值（平均值），这是事件 x 信息的理论最小编码大小。

上面的公式可以用期望值的形式重写如下：


这张图片展示了信息熵（Entropy）的数学定义及其不同表示形式。


$$
\text{Entropy} = \mathbb{E}_{x \sim P}[-\log P(x)]
$$
   
其中:

1. 随机变量 $ x $
2. 概率分布为 $ P $
3. 信息熵是 $ -\log P(x) $ 的期望值
4. $ \mathbb{E}_{x \sim P} $ 表示根据概率分布 $ P $ 计算期望值。


信息熵也可以用 $ H $ 来表示：
$$
H(P) = \mathbb{E}_{x \sim P}[-\log P(x)]
$$
其中 
1. $ H $ 表示熵
2. $ P $ 是概率分布。


简而言之，熵告诉我们遵循特定概率分布的事件的理论最小平均编码大小。

只要我们知道任何事物的概率分布，就能计算出它的熵。如果我们不知道概率分布，就无法计算熵。因此，我们需要估计概率分布。



## 估计熵(Estimating Entropy)

假设我们要向纽约报告东京的天气情况，我们希望将信息编码成尽可能小的大小。在天气发生之前，我们并不知道它的分布情况。为了便于讨论，我们假设在观察东京的天气一段时间后就能知道。
由于我们最初并不知道东京天气的概率分布，我们将其估计为 Q，以计算从东京向纽约传输天气报告的编码大小。
使用估计的概率分布 Q，估计的熵为:
$$
\text{EstimatedEntropy} = \mathbb{E}_{x \sim Q}[-\log Q(x)]
$$

如果 Q 接近真实的概率分布，上述估算就能告诉我们最小的平均编码大小是多少。然而，估算公式中存在两种不确定性。
如 x~Q 所示，我们使用估计的概率分布 Q 来计算期望值，而期望值将不同于实际的概率分布 P。
此外，我们根据估计的概率分布 Q 计算出的最小编码大小为-log Q。
由于估计的概率分布 Q 会影响期望值和编码大小的估计，因此估计的熵可能会非常错误。
另外，由于 Q 同时影响期望值计算和编码大小估计，它也可能因巧合而接近真实熵。
因此，将估计熵与实际熵进行比较可能毫无意义。

与克劳德-香农一样，我们主要关注的是如何使编码规模尽可能小。因此，我们希望将我们的编码大小与基于熵的理论最小编码大小进行比较。
如果我们在观察了东京一段时间的天气后得到了真实分布 P，我们就可以利用概率分布 P 和天气预报时使用的实际编码大小（基于 Q）计算出实现的平均编码大小。
它被称为 P 和 Q 之间的交叉熵，我们可以将其与熵进行比较。

$$
\text{CrossEntropy} = \mathbb{E}_{x \sim P}[-\log Q(x)]
$$

$$
\text{Entropy} = \mathbb{E}_{x \sim P}[-\log P(x)]
$$

我们是在比较苹果和苹果，因为我们在两种期望值计算中使用了相同的真实分布。我们比较的是理论上的最小编码和天气预报中实际使用的编码。简而言之，我们正在交叉检查编码大小，这就是交叉熵中 "交叉 "的含义。

## 交叉熵 ≥ 熵

$$
H(P,Q) = \mathbb{E}_{x \sim P}[-\log Q(x)]
$$


因此，H(P, Q) 和 H(Q, P) 不一定相同，除非当 Q=P 时，在这种情况下，H(P, Q) = H(P, P) = H(P)，它本身就是熵。这一点很微妙，但却至关重要。对于期望值，我们应该使用真实概率 P，因为它说明了事件的分布情况。对于编码大小，我们应该使用 Q，因为 Q 是用来对信息进行编码的。

由于熵是理论上的最小平均尺寸，因此交叉熵高于或等于熵，但不小于熵。换句话说，如果我们的估计是完美的，那么 Q = P，因此 H（P，Q）=H（P）。否则，H（P，Q）> H（P）。



## 作为损失函数的交叉熵

假设我们有一个动物图片数据集，其中有五种不同的动物。每张图片中只有一种动物。每张图像都使用单次编码标记相应的动物。

![alt text](交叉熵/1.png)


| Animal（动物） | Label（标签）   |
| -------------- | --------------- |
| Dog（狗）      | [1 0 0 0 0 0] |
| Fox（狐狸）     | [0 1 0 0 0 0] |
| Horse（马）     | [0 0 1 0 0 0] |
| Eagle（鹰）     | [0 0 0 1 0 0] |
| Squirrel（松鼠）| [0 0 0 0 1 0] |

我们可以将一种热编码视为每幅图像的概率分布。让我们来看几个例子。

第一张图片是狗的概率分布为 1.0（=100%）。

1. P_1(dog) = 1
1. P_1(fox) = 0
1. P_1(horse) = 0
1. P_1(eagle) = 0
1. P_1(squirrel) = 0

至于第二张图片，标签告诉我们这是一只狐狸，这一点百分之百确定。


1. P_2(dog) = 0
1. P_2(fox) = 1
1. P_2(horse) = 0
1. P_2(eagle) = 0
1. P_2(squirrel) = 0

因此，每幅图像的熵均为零。


H(P1) = 0
H(P2) = 0
H(P3) = 0
H(P4) = 0
H(P5) = 0


独热编码标签可以 100%确定地告诉我们每张图片上的动物是什么。

假设我们有一个机器学习模型来对这些图像进行分类。当我们没有对模型进行充分训练时，它可能会对第一幅图像（狗）进行如下分类：

Q_1 = [0.4 0.3 0.05 0.05 0.2]

第一幅图像中狗占 40%，狐狸占 30%，马占 5%，鹰占 5%，松鼠占 20%。这种估计对于第一幅图像中的动物并不精确，也不确定。

相比之下，标签则为我们提供了第一张图片动物类别的准确分布。它能百分之百地告诉我们这是一只狗。

P_1 = [1 0 0 0 0]

那么，模型的预测结果如何呢？我们可以如下计算交叉熵：

$$
\begin{align*}
H(P_{1},Q_{1})&=-\sum_{i}P_{1}(i)\log Q_{1}(i)\\
&=-(1\log 0.4 + 0\log 0.3 + 0\log 0.05 + 0\log 0.05 + 0\log 0.2)\\
&=-\log 0.4\\
&\approx 0.916
\end{align*}
$$

这比标签的零熵值要高，但我们并不能直观地理解这个值的含义。因此，我们再来看看另一个交叉熵值，以作比较。
模型训练有素后，可以对第一幅图像做出如下预测。
Q_1 = [0.98 0.01 0 0 0.01]

如下图所示，交叉熵比以前低得多。

$$
\begin{align*}
H(P_1, Q_1) &= -\sum_i P_1(i) \log Q_1(i) \\
&= -(1 \log 0.98 + 0 \log 0.01 + 0 \log 0 + 0 \log 0 + 0 \log 0.01) \\
&= -\log 0.98 \\
&\approx 0.02
\end{align*}
$$

交叉熵将模型的预测与标签（即真实的概率分布）进行比较。随着预测越来越准确，交叉熵会下降。如果预测是完美的，交叉熵就会变为零。因此，交叉熵可以作为训练分类模型的损失函数。



##  Nats vs. Bits

在机器学习中，我们使用基 e 而不是基 2 有多种原因（其中之一是便于计算导数）。改变对数基数不会造成任何问题，因为它只改变了大小。

$$
log_2 x = \frac{log_e x}{log_e 2}
$$

顺便提一下，以 e 为底对数的信息单位是纳特(nat)，而以 2 为底对数的信息单位称为比特(bit)。1 纳特的信息量来自以 1/e 的概率发生的事件。

$$
1 = -logP \\
P = \frac{1}{e}
$$

以 e 为底的对数不如以 2 为底的对数直观。1 比特的信息量来自以 1/2 概率发生的事件。如果我们能用 1 比特编码一条信息，那么一条这样的信息就能减少50%的不确定性。同样的类比在以 e 为底的情况下不易实现，这也是为什么人们经常使用以 2 为底的对数来解释信息熵概念的原因。不过，机器学习应用使用基 e 对数是为了执行方便。


## 二元交叉熵(Binary Cross-Entropy)
我们可以将二元交叉熵用于有是/否答案的二元分类。例如，图像中只有狗或猫。对于二元分类，交叉熵公式只包含两个概率：
$$
\begin{align*}
H(P, Q) &= - \sum_{i=(\text{cat}, \text{dog})} P(i) \log Q(i) \\
&= - P(\text{cat}) \log Q(\text{cat}) - P(\text{dog}) \log Q(\text{dog})
\end{align*}
$$
利用以下关系:

$$
P(dog) = (1-P(cat))
$$

我们可以将交叉熵写法如下：
$$
H(P,Q) = -P(cat)logQ(cat) - (1-P(cat))log(1-Q(cat))
$$

我们可以通过稍微改变符号来进一步简化它：

$$
P = P(\text{cat})
$$
$$
\hat{P} = Q(\text{cat})
$$

那么，二元交叉熵公式就变成了

$$
\text{BinaryCrossEntropy} = -P \log \hat{P} - (1 - P) \log(1 - \hat{P})
$$


简而言之，二元交叉熵就是包含两个类别的交叉熵。除此之外，它们是同一个概念。既然我们已经对这些概念了如指掌，那么对于我们所有人来说，交叉熵概念的熵值应该为零。



## 参考:
1. https://kikaben.com/cross-entropy-demystified/



































































































