

# OWE编码(WOEEncoder)


WOE（Weight of Evidence）叫做证据权重, 只能用于二进制目标变量，即0/1的目标变量。

1. 处理缺失值：当数据源没有100%覆盖时，那就会存在缺失值，此时可以把null单独作为一个分箱。这点在分数据源建模时非常有用，可以有效将覆盖率哪怕只有20%的数据源利用起来。
2. 处理异常值：当数据中存在离群点时，可以把其通过分箱离散化处理，从而提高变量的鲁棒性（抗干扰能力）。例如，age若出现200这种异常值，可分入“age > 60”这个分箱里，排除影响。
3. 业务解释性：我们习惯于线性判断变量的作用，当x越来越大，y就越来越大。但实际x与y之间经常存在着非线性关系，此时可经过WOE变换。

$$
WOE_i=ln(\frac{Bad_i}{Bad_T}/\frac{Good_i}{Good_T})=ln(\frac{Bad_i}{Bad_T})-ln(\frac{Good_i}{Good_T})
$$


计算WOE步骤：

1. 对于连续型变量，进行分箱（binning），可以选择等频、等距，或者自定义间隔；对于离散型变量，如果分箱太多，则进行分箱合并。
2. 统计每个分箱里的好人数(bin_goods)和坏人数(bin_bads)。
3. 分别除以总的好人数(total_goods)和坏人数(total_bads)，得到每个分箱内的边际好人占比(margin_good_rate)和边际坏人占比(margin_bad_rate)。
4. 计算每个分箱里的
5. 检查每个分箱（除null分箱外)里woe值是否满足单调性，若不满足，返回step1。注意⚠️：null分箱由于有明确的业务解释，因此不需要考虑满足单调性。
6. 计算每个分箱里的IV，最终求和，即得到最终的IV。

在WOEEncoder里：只有两种分布：

1. 1的分布（每组y=1的数量/所有y=1的数量）
2. 0的分布（每组y=0的数量/所有y=0的数量）

算法核心：对每个分组，将1的分布除以0的分布；这个值越高，越有信心偏向该组的1，反之亦然。

WOEEncoder的计算公式如下，每个特征取值$x_i$表示为$WoE_i$:

$$
\hat{P_1}=\frac{N_{y=1\;and\;x=i}}{N_{y=1}}\\
\hat{P_0}=\frac{N_{y=0\;and\;x=i}}{N_{y=0}}\\
WoE_i=ln\frac{\hat{P_1}}{\hat{P_0}}
$$




## pandas


## [category_encoders](https://contrib.scikit-learn.org/category_encoders/index.html)



```python
from sklearn.datasets import load_boston
bunch = load_boston()
y = bunch.target > 22.5
X = pd.DataFrame(bunch.data, columns=bunch.feature_names)
ce_15 = ce.WOEEncoder(cols=['CHAS', 'RAD']).fit_transform(X, y)
print('\nOriginal Dataset (Boston):\n', X)
print('\nWOEEncoder Return the transformed dataset:\n', ce_15)
```












## 参考
1. https://datascience.stackexchange.com/questions/39317/difference-between-ordinalencoder-and-labelencoder
2. https://www.cnblogs.com/dangui/p/15836197.html