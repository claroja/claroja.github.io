
## 二分类问题(binary)

sigmoid函数容易出现梯度消失等问题。sigmoid函数只在原点附近有比较大的梯度变化，在两端梯度变化很小，参数难以更新。


1. 如果模型输出为非互斥类别(即没有包含关系)，且可以同时选择多个类别，则采用Sigmoid函数计算该网络的原始输出值。例如BraTS数据集中核心肿瘤和增强肿瘤的分类
2. 如果模型输出为互斥类别，且只能选择一个类别，则采用Softmax函数计算该网络的原始输出值。例如MNIST数据集的分类


对于二分类而言, 以输入$x_1$为例:

Sigmoid函数: $\text{output}(x_1) = \frac{1}{1+e^{-x_1}}$
Softmax函数: $\text{output}(x_1) = \frac{e^{x_1}}{1+e{-(x_1 - x_2)}}$

在Softmax函数中, $(x_1 - x_2)$可以用$z_1$代替, 既Softmax函数可以写成:
$$
output(z_1) = \frac{1}{1+e{-x_1}}
$$
和Sigmoid函数一致, 所以理论上说是没有区别的.




当你用Sigmoid函数的时候，你的最后一层全连接层的神经元个数为1，而当你用Softmax函数的时候，你的最后一层全连接层的神经元个数是2。这个很好理解，因为Sigmoid函数只有是目标和不是目标之分，实际上只存在一类目标类，另外一个是背景类。而Softmax函数将目标分类为了二类，所以有两个神经元。这也是导致两者存在差异的主要原因。

Sigmoid函数针对两点分布提出。神经网络的输出经过它的转换，可以将数值压缩到(0,1)之间，得到的结果可以理解成分类成目标类别的概率P，而不分类到该类别的概率是(1 - P)，这也是典型的两点分布的形式。

Softmax函数本身针对多项分布提出，当类别数是2时，它退化为二项分布。而它和Sigmoid函数真正的区别就在——二项分布包含两个分类类别（姑且分别称为A和B），而两点分布其实是针对一个类别的概率分布，其对应的那个类别的分布直接由1-P得出。

简单点理解就是，Sigmoid函数，我们可以当作成它是对一个类别的“建模”，将该类别建模完成，另一个相对的类别就直接通过1减去得到。而softmax函数，是对两个类别建模，同样的，得到两个类别的概率之和是1。

神经网络在做二分类时，使用Softmax还是Sigmoid，做法其实有明显差别。由于Softmax是对两个类别（正反两类，通常定义为0/1的label）建模，所以对于NLP模型而言（比如泛BERT模型），Bert输出层需要通过一个nn.Linear()全连接层压缩至2维，然后接Softmax（Pytorch的做法，就是直接接上torch.nn.CrossEntropyLoss）；而Sigmoid只对一个类别建模（通常就是正确的那个类别），所以Bert输出层需要通过一个nn.Linear()全连接层压缩至1维，然后接Sigmoid（torch就是接torch.nn.BCEWithLogitsLoss）。

总而言之，Sotfmax和Sigmoid确实在二分类的情况下可以化为相同的数学表达形式，但并不意味着二者有一样的含义，而且二者的输入输出都是不同的。Sigmoid得到的结果是“分到正确类别的概率和未分到正确类别的概率”，Softmax得到的是“分到正确类别的概率和分到错误类别的概率”。

一种常见的错法（NLP中）：即错误地将Softmax和Sigmoid混为一谈，再把BERT输出层压缩至2维的情况下，却用Sigmoid对结果进行计算。这样我们得到的结果其意义是什么呢？

假设我们现在BERT输出层经nn.Linear()压缩后，得到一个二维的向量：[-0.9419267177581787, 1.944047451019287]

对应类别分别是(0,1)。我们经过Sigmoid运算得到：tensor([0.2805, 0.8748])

前者0.2805指的是分类类别为0的概率，0.8748指的是分类类别为1的概率。二者相互独立，可看作两次独立的实验（显然在这里不适用，因为0-1类别之间显然不是相互独立的两次伯努利事件）。所以显而易见的，二者加和并不等于1。

若用softmax进行计算，可得：

tensor([0.0529, 0.9471])

这里两者加和是1，才是正确的选择。


参考:
https://www.zhihu.com/question/295247085/answers/updated







## 多分类(Multiclass)

两个以上类的分类任务且每个样本只能标记为一个类。例如: 识别图片中包含苹果, 梨, 桔子中的哪一中, 且图片中有且只有一种水果.

比如, 苹果用0表示, 梨用1表示, 桔子用2表示
```python
import numpy as np
y = np.array([0, 1, 2])  # ['apple' 'pear' 'apple']
```

## 多标签分类(multilabel)

两个以上类的分类任务且每个样本可以标记为多个类。例如识别图片中包含苹果, 梨, 桔子中的哪一中, 且图片中可以包含多个水果.


比如, 第一章图片只包含苹果, 第二章图片包含苹果和橘子, 第三章图片都不包含
```python
import numpy as np
y = np.array([[1, 0, 0], [1, 0, 1], [0, 0, 0]])
print(y)
```

## 多输出回归(Multioutput regression)

预测每个样本的多个数值属性。每个属性都是一个数值变量，每个样本预测的属性数大于或等于2。例如预测苹果的体积和重量


比如, 第一个苹果体积是31.4, 重量是94
```python
y = np.array([[31.4, 94], [40.5, 109], [25.0, 30]])
```


## 多任务分类(Multiclass-multioutput classification)


属性的数量和类的数量都大于2. 对一组水果图像的属性“水果种类”和“颜色”进行分类。属性“水果种类”有可能的类别有：“苹果”，“梨”和“橙子”。属性“颜色”有可能的类别有：“绿色”、“红色”、“黄色”和“橙色”。每个样本都是一个水果的图像，两个属性的标签都是输出的，每个标签都是相应属性的可能类别之一。


```python
y = np.array([['apple', 'green'], ['orange', 'orange'], ['pear', 'green']])
```


One-Vs-Rest(one-vs-all): 在one-vs-all策略中，假设有n个类别，那么就会建立n个二项分类器，每个分类器针对其中一个类别和剩余类别进行分类。进行预测时，利用这n个二项分类器进行分类，得到数据属于当前类的概率，选择其中概率最大的一个类别作为最终的预测结果。
One-Vs-One: 在one-vs-one策略中，同样假设有n个类别，则会针对两两类别建立二项分类器，得到k=n*(n-1)/2个分类器。对新数据进行分类时，依次使用这k个分类器进行分类，每次分类相当于一次投票，分类结果是哪个就相当于对哪个类投了一票。在使用全部k个分类器进行分类后，相当于进行了k次投票，选择得票最多的那个类作为最终分类结果​。



## sigmoid softmax

如果你的任务需求允许，二者都可以。从二者公式来看（手机无法编辑公式，就不写了），softmax只比sigmoid多了一个∑=1的特性。如果你获得一个[0.9，0.6]的结果，也认为该类型是第0个标签，那sigmoid也没差什么，甚至还比softmax计算量更低。其余的交给loss和optimizer去逼近目标label即可，如果网络设计合理，数据也够用，那最终的模型都是符合要求的。在有一些非独立关系二分类问题上，sigmoid给出来的“好像都对”这种结果真的好像也对。激活函数嘛，只是将前值映射到一个目标值域的工具而已。


参考: https://www.zhihu.com/question/295247085/answers/updated



## 实践

在多分类问题中, 理论上一个样本的目标变量应该是数组, 如[0,1,0]表示正确的分类是第2种, 但是实际上我们的目标变量是2, 这是因为在框架已经帮我们做了转换.










## 参考
1. https://scikit-learn.org/stable/modules/multiclass.html
2. https://scikit-learn.org.cn/view/91.html