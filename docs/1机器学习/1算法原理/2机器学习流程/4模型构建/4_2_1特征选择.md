根据特征选择的形式又可以将特征选择方法分为3种：

1. Filter：过滤法，按照发散性或者相关性对各个特征进行评分，设定阈值或者待选择阈值的个数，选择特征。
2. Wrapper：包装法，根据目标函数（通常是预测效果评分），每次选择若干特征，或者排除若干特征。
Embedded：嵌入法，先使用某些机器学习的算法和模型进行训练，得到各个特征的权值系数，根据系数从大到小选择特征。类似于Filter方法，但是是通过训练来确定特征的优劣。

## Filter过滤方法


### [去掉方差较小的特征](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.VarianceThreshold.html#sklearn.feature_selection.VarianceThreshold)


假设某特征的特征值只有0和1，并且在所有输入样本中，95%的实例的该特征取值都是1，那就可以认为这个特征作用不大。如果100%都是1，那这个特征就没意义了。方差阈值（VarianceThreshold）删除了方差不满足某个阈值的所有特征。默认情况下，它会删除所有的零方差特性，也即，在所有样本中具有相同值的特征。当特征值都是离散型变量的时候这种方法才能用，如果是连续型变量，就需要将连续变量离散化之后才能用。


1. 最佳实践

    ```python
    from sklearn.feature_selection import VarianceThreshold
    X = [[0, 1, 3], [0, 2, 2], [0, 3, 1]]
    selector = VarianceThreshold()
    selector.fit_transform(X)
    ```

2. 构造参数

    ```python
    class sklearn.feature_selection.VarianceThreshold(
        threshold=0.0  # float, 方差小于等于该值的特征将被删除
    )
    ```

3. 对象属性

    1. variances_: array, shape (n_features,), 每个特征的方差
    2. n_features_in_: int, fit方法传入的特征数
    3. feature_names_in_: ndarray of shape (n_features_in_,), fit方法传入特征的名称

4. 对象方法

    1. fit(X,[,y]):从样本数据中学习方差
    2. transform(X): 执行特征选择（删除低于阙值的特征）
    3. fit_transform(X [,y]) 执行特征选择（删除低于阙值的特征）



### 基于单变量特征统计检验的方法

单变量特征选择的原理是分别单独的计算每个变量的某个统计指标，根据该指标来判断哪些指标重要，剔除那些不重要的指标。
1. 对于分类问题(y离散)，可采用：_卡方检验_，f_classif, mutual_info_classif，互信息
2. 对于回归问题(y连续)，可采用：_皮尔森相关系数_，f_regression, mutual_info_regression，最大信息系数




Sklearn给出了一些常用基于统计检验的函数用于特征选择操作：

1. SelectBest： 只保留 k 个最高分的特征；
2. SelectPercentile ：只保留用户指定百分比的最高得分的特征；
3. 使用常见的单变量统计检验：假正率SelectFpr，错误发现率selectFdr，或者总体错误率SelectFwe；
4. GenericUnivariateSelect： 通过结构化策略进行特征选择，通过超参数搜索估计器进行特征选择。

将特征输入到评分函数，返回一个单变量的f_score(F检验的值)或p-values(P值，假设检验中的一个标准，P-value用来和显著性水平作比较)，注意SelectKBest 和 SelectPercentile只有得分，没有p-value。


以SelectBest为例:

1. 最佳实践
    
    使用卡方检验选择两个最优特征：

    ```python
    from sklearn.datasets import load_iris
    from sklearn.feature_selection import SelectKBest
    from sklearn.feature_selection import chi2
    iris = load_iris()
    X, y = iris.data, iris.target


    X_new = SelectKBest(chi2, k=2).fit_transform(X, y)
    print(X_new)
    ```

    使用F值进行特征选择（无需给出阈值）

    ```python
    from sklearn.feature_selection import SelectKBest,f_classif
    X=[
        [1,2,3,4,5],
        [5,4,3,2,1],
        [3,3,3,3,3],
        [1,1,1,1,1]
    ]
    y=[0,1,0,1]
    print('before transform:\n',X)
    sel=SelectKBest(score_func=f_classif,k=3)
    sel.fit(X,y)  #计算统计指标，这里一定用到y
    ```


2. 构造参数

    ```python
    class sklearn.feature_selection.SelectKBest(
        score_func=<function f_classif>,    # callable, 输入是(X,y), 返回(scores, pvalues) 列表形式, 默认方法只能应用于分类任务
        *,                                  # 
        k=10                                # int or “all”, 筛选出多少个特征
    )
    ```



    score_func: 给出用于计算统计指标的方法 ，可取值为：
    1. sklearn.feature_selection.f_regression: 基于线性回归分析来计算统计指标，给出各 特征的回归系数，系数比较大的特征更重要。适用于回归问题
    2. sklearn.feature_selection.mutual_info_regression: 计算X和y之间的互信息，以便度量相关程度，适用于回归问题
    3. sklearn.feature_selection.chi2: 计算各g特征的卡方统计量，适用于分类问题
    4. sklearn.feature_selection.f_classif: 根据方差分析（ANOVA）的原理，以F-分布为依据，利用平方和与自由度所计算的祖居与组内均方估计出F值，适用于分类问题
    5. sklearn.feature_selection.mutual_info_classif： 互信息，适用于分类问题。




1. 对象属性
    1. scores_: array-like of shape (n_features,)
    2. pvalues_: array-like of shape (n_features,)
    3. n_features_in_: int
    4. feature_names_in_: ndarray of shape (n_features_in_,)




### Pearson相关系数 (Pearson Correlation)

皮尔森相关系数是一种最简单的，能帮助理解特征和响应变量之间关系的方法，该方法衡量的是变量之间的线性相关性，结果的取值区间为[-1，1]，-1表示完全的负相关，+1表示完全的正相关，0表示没有线性相关。

```python
import numpy as np
from scipy.stats import pearsonr
np.random.seed(0)
size = 300
x = np.random.normal(0, 1, size)
y=x+np.random.normal(0, 1, size) #模拟y

# 输入：特征矩阵和目标向量
# 输出： 相关系数值和p值
print("Pearson相关系数:", pearsonr(x, y))
```

Pearson相关系数的一个明显缺陷是，作为特征排序机制，他只对线性关系敏感。如果关系是非线性的，即便两个变量具有一一对应的关系，Pearson相关性也可能会接近0。例如：

```python
x = np.random.uniform(-1,1,100000)
pearsonr(x,x**2)[0]
```



### 互信息和最大信息系数 Mutual information and maximal information coefficient (MIC)


经典的互信息（互信息为随机变量X与Y之间的互信息I(X;Y)为单个事件之间互信息的数学期望）也是评价定性自变量对定性因变量的相关性的，互信息计算公式如下：

$$
I(X;Y)=E[I(x_i;y_j)]=\sum_{ x_i\epsilon X }\sum_{ y_j\epsilon Y } p(x_i, y_j)log\frac{p(x_i,y_j)}{p(x_i)p(y_j)}
$$

互信息直接用于特征选择其实不是太方便：1、它不属于度量方式，也没有办法归一化，在不同数据及上的结果无法做比较；2、对于连续变量的计算不是很方便（X和Y都是集合，x，y都是离散的取值），通常变量需要先离散化，而互信息的结果对离散化的方式很敏感。


最大信息系数克服了这两个问题。它首先寻找一种最优的离散化方式，然后把互信息取值转换成一种度量方式，取值区间在[0，1]。 minepy 提供了MIC功能。


反过头来看$y=x^2$这个例子，MIC算出来的互信息值为1(最大的取值)。
```python
import numpy as np
from minepy import MINE
m = MINE()
x = np.random.uniform(-1, 1, 10000)
m.compute_score(x, x**2)
print(m.mic())
```

但是，MIC的统计能力遭到了一些质疑，当零假设不成立时，MIC的统计就会受到影响。在有的数据集上不存在这个问题，但有的数据集上就存在这个问题。









### 基于模型的特征排序 (Model based ranking)

这种方法的思路是直接使用你要用的机器学习算法，针对 每个单独的特征 和 响应变量建立预测模型。假如 特征 和 响应变量 之间的关系是非线性的，可以用基于树的方法(决策树、随机森林)、或者 扩展的线性模型 等。基于树的方法比较易于使用，因为他们对非线性关系的建模比较好，并且不需要太多的调试。但要注意过拟合问题，因此树的深度最好不要太大，再就是运用交叉验证。


在 波士顿房价数据集 上使用sklearn的 随机森林回归 给出一个_单变量选择_的例子(这里使用了交叉验证)：


```python
from sklearn.model_selection import cross_val_score, ShuffleSplit
from sklearn.datasets import load_boston
from sklearn.ensemble import RandomForestRegressor
import numpy as np

# Load boston housing dataset as an example
boston = load_boston()
X = boston["data"]
Y = boston["target"]
names = boston["feature_names"]

rf = RandomForestRegressor(n_estimators=20, max_depth=4)
scores = []
# 单独采用每个特征进行建模，并进行交叉验证
for i in range(X.shape[1]):
    score = cross_val_score(rf, X[:, i:i+1], Y, scoring="r2",  # 注意X[:, i]和X[:, i:i+1]的区别
                            cv=ShuffleSplit(len(X), 3, .3))
    scores.append((format(np.mean(score), '.3f'), names[i]))
print(sorted(scores, reverse=True))
```


## Wrapper方法

递归特征消除 (Recursive Feature Elimination)

递归消除特征法使用一个基模型来进行多轮训练，每轮训练后，移除若干权值系数的特征，再基于新的特征集进行下一轮训练。

sklearn官方解释：对特征含有权重的预测模型(例如，线性模型对应参数coefficients)，RFE通过递归减少考察的特征集规模来选择特征。首先，预测模型在原始特征上训练，每个特征指定一个权重。之后，那些拥有最小绝对值权重的特征被踢出特征集。如此往复递归，直至剩余的特征数量达到所需的特征数量。

RFECV 通过交叉验证的方式执行RFE，以此来选择最佳数量的特征：对于一个数量为d的feature的集合，他的所有的子集的个数是2的d次方减1(包含空集)。指定一个外部的学习算法，比如SVM之类的。通过该算法计算所有子集的validation error。选择error最小的那个子集作为所挑选的特征。明显缺点: 类似于穷尽搜索，成本过大。


```python
from sklearn.feature_selection import RFE
from sklearn.linear_model import LogisticRegression

#递归特征消除法，返回特征选择后的数据
#参数estimator为基模型
#参数n_features_to_select为选择的特征个数
RFE(estimator=LogisticRegression(), n_features_to_select=2).fit_transform(iris.data, iris.target)
```





## Embedded方法

使用SelectFromModel选择特征 (Feature selection using SelectFromModel)

有些机器学习方法本身就具有对特征进行打分的机制，或者很容易将其运用到特征选择任务中，例如回归模型，SVM，决策树，随机森林等等。其实Pearson相关系数等价于线性回归里的标准化回归系数。

SelectFromModel 作为meta-transformer，能够用于拟合后任何拥有coef_或feature_importances_属性的预测模型。 如果特征对应的coef_或feature_importances_值低于设定的阈值threshold，那么这些特征将被移除。除了手动设置阈值，也可通过字符串参数调用内置的启发式算法(heuristics)来设置阈值，包括：平均值(“mean”), 中位数(“median”)以及他们与浮点数的乘积，如”0.1*mean”。

### 基于L1的特征选择 (L1-based feature selection)



使用L1范数作为惩罚项的线性模型(Linear models)会得到稀疏解：大部分特征对应的系数为0。当你希望减少特征的维度以用于其它分类器时，可以通过 feature_selection.SelectFromModel 来选择不为0的系数。特别指出，常用于此目的的稀疏预测模型有 linear_model.Lasso（回归）， linear_model.LogisticRegression 和 svm.LinearSVC（分类）:


```python
from sklearn.svm import LinearSVC
from sklearn.datasets import load_iris
from sklearn.feature_selection import SelectFromModel
iris = load_iris()
X, y = iris.data, iris.target
X.shape  # (150, 4)
lsvc = LinearSVC(C=0.01, penalty="l1", dual=False).fit(X, y)
model = SelectFromModel(lsvc, prefit=True)
X_new = model.transform(X)
X_new.shape  # (150, 3)

```
使用feature_selection库的SelectFromModel类结合带L1以及L2惩罚项的逻辑回归模型:

```python
from sklearn.feature_selection import SelectFromModel
#带L1和L2惩罚项的逻辑回归作为基模型的特征选择
#参数threshold为权值系数之差的阈值
SelectFromModel(LR(threshold=0.5, C=0.1)).fit_transform(iris.data, iris.target)
```






### 回归模型和SVM

使用L1范数作为惩罚项的线性模型(Linear models)会得到稀疏解：大部分特征对应的系数为0。当你希望减少特征的维度以用于其它分类器时，可以通过 feature_selection.SelectFromModel 来选择不为0的系数。特别指出，常用于此目的的稀疏预测模型有 linear_model.Lasso（回归）， linear_model.LogisticRegression 和 svm.LinearSVC（分类）:

```python
from sklearn.svm import LinearSVC
from sklearn.datasets import load_iris
from sklearn.feature_selection import SelectFromModel
iris = load_iris()
X, y = iris.data, iris.target

lsvc = LinearSVC(C=0.01, penalty="l1", dual=False).fit(X, y)
model = SelectFromModel(lsvc, prefit=True)
X_new = model.transform(X)
```

在SVM和Logistic回归中，参数 C 控制着稀疏性，C越小选择的特征越少。在Lasso中，参数 alpha越大，选择的特征越少。



### 基于树的特征选择 (Tree-based feature selection)

基于树的预测模型（见 sklearn.tree 模块，森林见 sklearn.ensemble 模块）能够用来计算特征的重要程度，因此能用来去除不相关的特征（结合 sklearn.feature_selection.SelectFromModel）:

```python
from sklearn.ensemble import ExtraTreesClassifier
from sklearn.datasets import load_iris
from sklearn.feature_selection import SelectFromModel
iris = load_iris()
X, y = iris.data, iris.target

clf = ExtraTreesClassifier()
clf = clf.fit(X, y)

model = SelectFromModel(clf, prefit=True)
X_new = model.transform(X)

#输出特征重要性值
print(clf.feature_importances_  )
print(X_new)
```




## 参考
1. https://zhuanlan.zhihu.com/p/141506312
2. https://www.cnblogs.com/stevenlk/p/6543628.html
3. https://www.cnblogs.com/jasonfreak/p/5448385.html#3601031
4. https://zhuanlan.zhihu.com/p/141506312












