

## 接口


### 构造参数

```python
BayesSearchCV(
    estimator=,                 # scikitlearn模型, 
    search_spaces,              # 字典, 参数空间, 等价于GridSearchCV的params_grid
    optimizer_kwargs=None,      # 
    n_iter=50,                  # 整型, 迭代次数
    scoring=None,               # 评分
    fit_params=None, 
    n_jobs=1,                   # 整型, 并行
    n_points=1, 
    iid='deprecated', 
    refit=True,                 # 布尔, 训练最好的参数模型
    cv=None,                    # 整型, 交叉验证的K值
    verbose=0,                  # 整型, 说明的详细说明
    pre_dispatch='2*n_jobs', 
    random_state=None,          # 整型, 随机种子
    error_score='raise', 
    return_train_score=False)

```
### 对象属性与方法

属性或方法|类型|说明
--|--|--
best_estimator_|模型|最优的模型, 仅在参数refit=True时可用
best_score_|float|最优模型的交叉验证平均分
best_params_|dict|最优模型的超参, 仅在参数refit=True时可用



### search_spaces定义

```python
Real(
    low,                # float, 下边界(闭区间)
    high,               # float, 上边界(闭区间)
    prior='uniform',    # string, uniform: 均匀分布[lower, base]; log-uniform均匀分布[log(lower, base), log(upper, base)]
    base=10,            # int, prior='log-uniform'时的base.
    transform=None, 
    name=None, 
    dtype=<class 'float'>)

```

```python
Integer(
    low,                # float, 下边界(闭区间)
    high,               # float, 上边界(闭区间)
    prior='uniform',    # string, uniform: 均匀分布[lower, base]; log-uniform均匀分布[log(lower, base), log(upper, base)]
    base=10,            # int, prior='log-uniform'时的base.
    transform=None, 
    name=None, 
    dtype=<class 'numpy.int64'>)
```


```python
Categorical(
    categories,         # 列表, 可能分类的取值
    prior=None,         # 列表, 每个可能分类取值的权重
    transform=None, 
    name=None)
```

## 实战

```python
from sklearn.datasets import load_breast_cancer 
from sklearn.neighbors import KNeighborsClassifier
from skopt import BayesSearchCV
from skopt.space import Categorical, Integer
from sklearn.model_selection import train_test_split 
from sklearn.metrics import classification_report


dataset = load_breast_cancer()
X=dataset.data
Y=dataset.target

X_train, X_test, y_train, y_test = train_test_split( 
    X,Y,test_size = 0.2, random_state = 10) 

params_default = {
    'n_neighbors': 5,
    'weights': 'uniform',
    'p': 2
}

model = KNeighborsClassifier(**params_default)
model.fit(X_train, y_train) 

predictions = model.predict(X_test) 
print(classification_report(y_test, predictions)) 

#               precision    recall  f1-score   support
# 
#            0       0.89      0.87      0.88        39
#            1       0.93      0.95      0.94        75
# 
#     accuracy                           0.92       114
#    macro avg       0.91      0.91      0.91       114
# weighted avg       0.92      0.92      0.92       114




params = {
    'n_neighbors': Integer(1,20),
    'weights': Categorical(['uniform', 'distance']),
    'p': Integer(1,2)
}

clf_grid = BayesSearchCV(
    estimator=KNeighborsClassifier(),
    search_spaces=params,
    n_iter=100,
    cv=5,
    n_jobs=5,
    verbose=0
)

clf_grid.fit(X_train, y_train) 

print(clf_grid.best_params_)  # ('n_neighbors', 8), ('p', 1), ('weights', 'uniform')
grid_predictions = clf_grid.predict(X_test) 

print(classification_report(y_test, grid_predictions)) 

#               precision    recall  f1-score   support
# 
#            0       0.92      0.92      0.92        39
#            1       0.96      0.96      0.96        75
# 
#     accuracy                           0.95       114
#    macro avg       0.94      0.94      0.94       114
# weighted avg       0.95      0.95      0.95       114
```













## 参考
1. https://scikit-optimize.readthedocs.io/en/stable/modules/generated/skopt.BayesSearchCV.html
1. https://www.geeksforgeeks.org/hyperparameter-optimization-based-on-bayesian-optimization/
2. https://machinelearningmastery.com/scikit-optimize-for-hyperparameter-tuning-in-machine-learning/
3. https://inside-machinelearning.com/en/bayesian-optimization/
4. https://scikit-learn.org/stable/modules/grid_search.html
5. http://blairhudson.com/blog/posts/optimising-hyper-parameters-efficiently-with-scikit-optimize/
6. https://www.kaggle.com/code/lucamassaron/tutorial-bayesian-optimization-with-xgboost
7. https://keramatfar-a-s.medium.com/stop-using-gridsearchcv-c298afaa65cf
8. https://www.cnblogs.com/wmx24/p/10025600.html
9. https://towardsdatascience.com/automated-machine-learning-hyperparameter-tuning-in-python-dfda59b72f8a