# autoClasses

## åˆå§‹åŒ–æ¨¡å‹

`from_retrained()`æ–¹æ³•é€šè¿‡pretrained modelçš„nameå’Œpathæ¥åŠ è½½æ¨¡å‹
```python
model = AutoModel.from_pretrained("bert-base-cased")
```
ä»¥ä¸Šä»£ç åŠ è½½äº†[BertModel](https://huggingface.co/docs/transformers/v4.20.1/en/model_doc/bert#transformers.BertModel), ä¸»è¦åŒ…å«äº†

BertConfig: å­˜å‚¨äº†bertæ¨¡å‹çš„é…ç½®.æ¯”å¦‚: vocab_size(è¯­æ–™åº“çš„å¤§å°, å†³å®šäº†inputs_idsçš„èŒƒå›´),hidden_size(éšè—å±‚çš„å¤§å°),num_hidden_layers(éšè—å±‚çš„å±‚æ•°)ç­‰.[å‚è€ƒå®˜ç½‘](https://huggingface.co/docs/transformers/v4.20.1/en/model_doc/bert#transformers.BertConfig)
BertModel: è¾“å‡ºraw hidden-states, æ²¡æœ‰æœ€ç»ˆçš„softmaxç­‰. æ˜¯`torch.nn.Module`çš„å­ç±», å¯ä»¥åƒ`pytorch`ä¸€æ ·æ“ä½œ. [å‚è€ƒå®˜ç½‘](https://huggingface.co/docs/transformers/v4.20.1/en/model_doc/bert#transformers.BertModel)

1. åŠ è½½æ¨¡å‹é…ç½®

```python
from transformers import AutoConfig
config = AutoConfig.from_pretrained("bert-base-uncased")
```

2. åŠ è½½model

```python
from transformers import AutoConfig, AutoModel
config = AutoConfig.from_pretrained("bert-base-uncased")
model = AutoModel.from_config(config)
```


## åˆå§‹åŒ– Tokenizer

BertTokenizer: åœ¨pretrained model vocabularyåˆå§‹åŒ–åˆ¶ä½œtokençš„ç±». [å‚è€ƒå®˜ç½‘](https://huggingface.co/docs/transformers/v4.20.1/en/model_doc/bert#transformers.BertTokenizer)

```python
from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
```

## è¾“å‡ºç»“æœ

```python
from transformers import AutoModelForSequenceClassification
pt_batch = tokenizer(
    ["We are very happy to show you the ğŸ¤— Transformers library.", "We hope you don't hate it."],
    padding=True,
    truncation=True,
    max_length=512,
    return_tensors="pt",
)
model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
pt_model = AutoModelForSequenceClassification.from_pretrained(model_name)
pt_outputs = pt_model(**pt_batch)
from torch import nn
pt_predictions = nn.functional.softmax(pt_outputs.logits, dim=-1)
print(pt_predictions)
## tensor([[0.0021, 0.0018, 0.0115, 0.2121, 0.7725],
##         [0.2084, 0.1826, 0.1969, 0.1755, 0.2365]], grad_fn=<SoftmaxBackward0>)
```



## automodelforsequenceclassification-vs-automodel


å»ºè®®ä½¿ç”¨`AutoTokenizer`ç±»å’Œ`AutoModelFor`ç±»æ¥åŠ è½½æ¨¡å‹, è€Œä¸æ˜¯ä½¿ç”¨`AutoModel`ç±»æ¥åŠ è½½.
[å‚è€ƒå®˜ç½‘](https://huggingface.co/docs/transformers/autoclass_tutorial#automodel)

https://stackoverflow.com/questions/69907682/what-are-differences-between-automodelforsequenceclassification-vs-automodel

å‚è€ƒ:
https://huggingface.co/docs/transformers/v4.20.1/en/model_doc/bert
https://huggingface.co/docs/transformers/model_doc/auto