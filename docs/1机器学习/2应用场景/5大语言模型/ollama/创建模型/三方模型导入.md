# 第三方模型

ollama除了内置模型外, 还可以导入下列格式的模型:
1. GGUF
2. PyTorch
3. Safetensors



## GGUF
[GGUF](https://huggingface.co/docs/hub/en/gguf)是[GGML](https://huggingface.co/blog/zh/introduction-to-ggml)格式的模型.


Ollama 导入 GGUF 模型的步骤:
1. 创建一个名为 'Modelfile' 的文件，其中包含 'FROM' 指令和要导入的模型的本地文件路径。

    ```sh
    FROM /path/to/your/vicuna-33b.Q4_0.gguf
    ```

2. 在 Ollama 中创建模型, 并指定名称.

    ```sh
    ollama create vicuna-33b-q4 -f Modelfile
    ```

3. 运行指定名称的模型

    ```sh
    ollama run vicuna-33b-q4
    ```


## 从PyTorch 或 Safetensors导入

如果要导入的模型使用以下架构之一，则可以使用 Modelfile 将其直接导入 Ollama：

1. LlamaForCausalLM
2. MistralForCausalLM
3. GemmaForCausalLM

```sh
FROM /path/to/safetensors/your/directory
```


## 其他不支持格式, 先转换为GGUF


使用 llama.cpp 将转换为 GGUF 格式, 支持 HuggingFace、GGML 和 Lora 等常见格式，并提供转换脚本。例如，转换 HuggingFace 模型。

1. 克隆 llama.cpp 存储库

    ```shell
    git clone https://github.com/ggerganov/llama.cpp.git
    ```

2. 安装必要的Python依赖项：


    ```shell
    pip install -r llama.cpp/requirements.txt
    ```

    ```shell
    python convert.py --input /path/to/huggingface/model --output /path/to/output/model.gguf
    ```


## 参考
1. https://medium.com/@tubelwj/complete-guide-to-running-ollamas-large-language-model-llm-locally-part-1-97f936da4eb0



































