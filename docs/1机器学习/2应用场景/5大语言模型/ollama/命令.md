
# Ollama命令行

Ollama命令类似于Docker.
1. Ollama的权重文件model类似Docker中的Image文件, 使用`create`, `pull`, `show`, `list`, `rm`控制
2. Ollama和Docker都有容器(container)的概念, 使用`serve`, `run`, `ps`, `stop`控制

具体的命令说明如下:

操作对象|命令|中文翻译|
|---- | ---- | ---- |
|模型|ollama create new_model|基于modelfile创建新模型|
|模型|ollama pull model|将指定的模型下载到系统中|
|模型|ollama show model|显示特定模型的详细信息，如配置和发布日期|
|模型|ollama list|列出所有已下载的模型|
|模型|ollama rm model|从系统中删除指定的模型| 
|容器|ollama serve|在本地系统启动Ollama|
|容器|ollama run model|运行指定的模型，使其可用于交互|
|容器|ollama ps|显示当前正在运行的模型|
|容器|ollama stop model|停止指定的正在运行的模型|



## serve


1. Usage:
  
    `ollama serve [flags]`

2. Flags: 

3. Environment Variables:
    1. OLLAMA_DEBUG               debug模式 (e.g. OLLAMA_DEBUG=1)
    2. OLLAMA_HOST                默认127.0.0.1:11434, 如果开启远程访问改为0.0.0.0:11434
    3. OLLAMA_KEEP_ALIVE          模型在内存的持续时间 (e.g. 5m)
    4. OLLAMA_MAX_LOADED_MODELS   单块GPU最大加载模型的数量
    5. OLLAMA_MAX_QUEUE           设置请求队列的最大长度。默认为 512。此变量用于控制并发请求的数量，避免过多请求同时处理导致服务过载
    6. OLLAMA_MODELS              指定模型文件的存储路径。默认为用户主目录下的 .ollama/models 文件夹。
    7. OLLAMA_NUM_PARALLEL       设置同时处理的并行请求数量。默认为 0，表示不限制。此变量用于优化服务的并发处理能力
    8. OLLAMA_NOPRUNE            在启动时不清理模型文件。默认为 false。启用此变量可以保留所有模型文件，避免不必要的清理操作
    9. OLLAMA_ORIGINS            配置允许跨域请求的来源列表。默认包含 localhost、127.0.0.1、0.0.0.0 等本地地址以及一些特定协议的来源。通过设置此变量，可以指定哪些来源可以访问 Ollama 服务，例如 OLLAMA_ORIGINS=*,https://example.com 允许所有来源以及 https://example.com 的跨域请求。
    10. OLLAMA_SCHED_SPREAD       允许模型跨所有 GPU 进行调度。默认为 false。启用此变量可以提高模型运行的灵活性和资源利用率
    11. OLLAMA_FLASH_ATTENTION    用于测试和使用新的注意力机制特性
    12. OLLAMA_KV_CACHE_TYPE       Quantization type for the K/V cache (default: f16)
    13. OLLAMA_LLM_LIBRARY         Set LLM library to bypass autodetection
    14. OLLAMA_GPU_OVERHEAD        为每个 GPU 预留的显存（以字节为单位）, 用于确保每个 GPU 有一定的显存余量，避免显存不足导致的问题
    15. OLLAMA_LOAD_TIMEOUT        设置模型加载过程中的超时时间。此变量用于防止模型加载过程过长导致服务无响应. 默认5分钟


✨Ollama在win中安装后自动启动且开机自动运行, 所以不用再使用serve命令



## run
通过 run 命令的方式与大模型交互，具有以下的优点：

1. 直接在终端运行 run 命令，可便捷高效地验证本地部署的大模型的响应效果
2. 记录保存大模型的响应：可将大模型的响应记录到本地文件。
3. 基于 run 命令开发自动化执行的脚本，定时调度执行，实现与大模型交互的自动化工作流程。





1. Usage:
  
  `ollama run MODEL [PROMPT] [flags]`

2. Flags:
    1. --format string     使用的模板 (e.g. json)
    2. --insecure           Use an insecure registry
    3. --keepalive string   模型在内存的持续时间 (e.g. 5m)
    4. --nowordwrap         不折叠多行
    5. --verbose            冗余的信息

2. Environment Variables:
    OLLAMA_HOST            run方法会启动HTTP服务(默认127.0.0.1:11434, 如果开启远程访问改为0.0.0.0:11434)
    OLLAMA_NOHISTORY       启用此变量可以避免保存命令历史记录, 默认为 false














## 参考
1. https://datawhalechina.github.io/handy-ollama/#/C1/1.%20Ollama%20%E4%BB%8B%E7%BB%8D
2. https://zhuanlan.zhihu.com/p/23824503091