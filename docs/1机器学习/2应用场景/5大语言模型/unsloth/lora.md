
LoRA（Low-Rank Adaptation）降低微调过程中的计算和内存需求的大型语言模型的微调技术.

在LoRA中，原始模型的全秩矩阵被分解为低秩矩阵的乘积。具体来说，对于一个全秩矩阵W，LoRA将其分解为两个低秩矩阵A和B的乘积，即W ≈ A * B。其中，A和B的秩远小于W的秩，从而显著减少了参数数量。


$$
W' = W + \frac{\alpha}{r} A\cdot B
$$

其中
1. W是原始的权重矩阵
2. A是一个尺寸为dr的矩阵
3. B是一个尺寸为rd的矩阵
4. r是低秩矩阵的秩
5. $\alpha$是LoRA引入了一个缩放因子


peft（Parameter-Efficient Fine-Tuning）它通过引入LoRA、Adapter等技术，使得只需调整部分参数即可实现有效的微调。


## 参数

1. target_modules指定模型中需要插入低秩矩阵调整的模块。

    以Deepseek为例:
    
    1. Attention层: 
        1. 查询（q_proj）
        2. 键（k_proj）
        3. 值（v_proj）
        4. 输出（o_proj）投影矩阵
    2. MLP层: 
        1. gate_proj：控制门投影(MOE专家系统)
        2. up_proj：上升投影 
        3. down_proj：下降投影 

2. r、alpha、dropout
    1. r: 低秩矩阵的秩值。r决定了微调时使用的低秩矩阵的维度，较小的r可以减少参数数量，从而提高训练效率，但可能牺牲一定的模型表现。较小的r（例如 8-32）适用于较小模型或需要较低资源的情况，而较大的r（例如 64-128）适用于更大规模的模型。
    2. alpha: 用来控制低秩矩阵的缩放因子。通过调整alpha，可以平衡低秩矩阵的影响，使模型能够在微调过程中保持足够的表达能力。16-32 是比较常见的选择，较大的alpha值通常会增加模型的表达能力，但也可能增加训练难度。
    3. Dropout: 正则化技术，通过在训练过程中随机丢弃神经网络中的部分神经元来防止过拟合。dropout率控制丢弃的概率，较高的dropout率有助于减少模型的复杂度，从而提升其在新数据上的泛化能力。对于大多数任务，0.2-0.3 是比较常见的取值，较低的dropout值（如 0.1）适合于较小的模型，而较高的dropout值（如 0.4-0.5）适合于较大的网络，尤其是在防止过拟合时。

3. bias: 指定 LoRA 微调时如何处理偏置（bias）项。具体来说，这个参数控制了在低秩适应中，是否保留或者修改偏置项。LoRA微调一般会将权重矩阵拆分成低秩矩阵来减少训练时的计算开销，但偏置项通常会保留或处理得不同。

    1. "none"：不对偏置项进行微调，也就是说，偏置项保持原样，不参与LoRA的低秩适应过程。这是默认选项，表示不修改偏置项，保持原有权重。
    2. "all"：对所有的偏置项进行微调，这意味着LoRA不仅会对权重矩阵进行低秩适应，还会对偏置项进行相应的调整。
    3. "lora_only"：仅对LoRA引入的低秩矩阵中的偏置项进行微调。即在LoRA的低秩变换部分，偏置项会被包含在内，并进行优化。

4. use_rslora (bool): 是否使用Rank-Stabilized LoRA。这个方法有时可以提高训练效果，尤其是在低秩情况下。若不需要，可以保持False。

5. init_lora_weights (bool | Literal["gaussian", "olora", "pissa", "loftq"]): 初始化LoRA权重的方式。常用的初始化方式是"gaussian".

6. modules_to_save (list[str]): 除LoRA层外需要保存的其他模块，通常用于在分类任务中保存最后的分类层等。

7. layers_to_transform (list[int] | int): 选择要转换的层，适用于大规模模型，可以选择特定层进行微调。默认会选择整个模型进行LoRA微调。

8. layers_pattern (list[str] | str): 用于指定层模式名称，与layers_to_transform结合使用，用于选择模型中的特定层。

3. task_type
    1. CAUSAL_LM: 自回归语言建模任务，模型基于输入的部分文本（上下文）来预测下一个词，适用于生成任务，如文本生成和语言建模。
    2. SEQ_CLS: 文本分类任务，模型将整个输入文本分类到某个类别。常见的应用包括情感分析、垃圾邮件检测、新闻分类等。
    3. SEQ_2_SEQ_LM:序列到序列的语言建模任务。该任务类型处理输入序列并生成一个输出序列。通常用于机器翻译、文本摘要等任务。
    4. TOKEN_CLS:标记分类任务，模型为输入文本的每个标记（通常是词或子词）分配一个类别标签。常见应用包括命名实体识别（NER）、词性标注（POS）、依存句法分析等。
    5. QUESTION_ANS: 问答任务，模型根据输入的问题和上下文，提取答案。常见应用包括阅读理解、基于文档的问答等。
    6. FEATURE_EXTRACTION: 特征提取任务，模型提取输入数据的隐藏状态（通常是编码器的输出），这些隐藏状态可以用于下游任务，如聚类、分类或作为其他任务的输入特征。比如给定一段文本，模型输出该文本的向量表示，这些向量可以用于情感分析、推荐系统或相似度计算等任务。


## 参考

https://zhuanlan.zhihu.com/p/15084326626