
名称|描述
--|--
V3|PT+SFT+RL, MoE
R1-Zero|纯RL（无SFT）, 纯RL驱动的V3基座模型
R1|PT+SFT+RL, 基于V3的RL优化架构
R1-Distill-Qwen|阿里通义千问蒸馏
R1-Distill-Llama|Meta大语言模型蒸馏


✨
1. SFT: Supervised Fine-Tuning
2. RL: Reinforcement Learning
3. PT: Pre-training

## 参考
1. https://deepseek.csdn.net/67aae53382931a478c542a56.html
2. https://www.cnblogs.com/ghj1976/p/18699261/jie-du-deepseekr1zero-he-deepseekr1-de-qian-shi-ji
3. https://ai.plainenglish.io/deepseek-r1-understanding-grpo-and-multi-stage-training-5e0bbc28a281
4. https://medium.com/data-science-in-your-pocket/what-is-grpo-the-rl-algorithm-used-to-train-deepseek-12acc19798d3