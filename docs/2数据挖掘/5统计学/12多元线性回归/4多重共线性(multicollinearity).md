# 多重共线性

多重共线性（Multicollinearity）是指线性回归模型中的解释变量之间由于存在精确相关关系或高度相关关系而使模型估计失真或难以估计准确。

因为多重共线性其实指的是自变量之间存在线性关系，既然是之间，那至少得有两个自变量，两个及两个以上即为多，所以称之为多重。如果含两个及以上自变量的Logistic回归被称为多重Logistic回归（一般叫法是多因素Logistic回归），含两个及以上自变量的Cox回归被称为多重Cox回归(一般叫做多因素Cox回归)，那么和多重线性回归一样，多重Logistic回归，多重Cox回归中也可能存在多重共线性的问题。

## 线代理解多重共线性

假设有100条数据(样本量$n=100$), 因变量为$Y$, 共有$p-1$个自变量, 分别为$X_1, X_2, ..., X_{p-1}$. 现在需要用$X_1, X_2, ..., X_{p-1}$来预测$Y$. 于是构建多重线性回归模型:
$$
Y_i = \beta_0 + \beta_1X_{i1} + ... + \beta_{p-1}X_{i,p-1} + \epsilon_i(i=1,2,...,100)
$$

从线性角度理解:

$$
Y=\begin{pmatrix} Y_1 \\ Y_2 \\ . \\ . \\ . \\ Y_{100} \end{pmatrix}
$$

$$
Y=\begin{pmatrix} 
1 & X_{1,1} & X_{1,2} & ... & X_{1, p-1} \\  
1 & X_{2,1} & X_{2,2} & ... & X_{2, p-1} \\ 
. & . & . & . & . \\ 
. & . & . & . & . \\ 
. & . & . & . & . \\  
1 & X_{100,1} & X_{100,2} & ... & X_{100, p-1} \end{pmatrix}
$$


$$
\beta=\begin{pmatrix} \beta_1 \\ \beta_2 \\ . \\ . \\ . \\ \beta_{p-1} \end{pmatrix}
$$

$$
\epsilon=\begin{pmatrix} \epsilon_1 \\ \epsilon_2 \\ . \\ . \\ . \\ \epsilon_{100} \end{pmatrix}
$$

则上述多重共线性模型也可以记作$Y=X\beta + \epsilon$, 其中$Y$的维度为$100*1$, $X$的维度为$100*p$, $\beta$的维度为$p*1$, $\epsilon$的维度为$100*1$. 按照最小二乘估计(leaste square estimate, lse), 使得误差平方和($Q$)最小即可得$\beta$的估计值.

> 这里$\beta$是未知参量, $X$是变换矩阵, 所以写成$X\beta$

$$
Q = \sum_{n-1}^{100}(Y-\beta_0-\beta_1X_{i1} - ... - \beta_{p-1}X_{i,p-1})^2
$$

求$\beta$的最小二乘估计$b$的矩阵表示为:

$$
b = (X^TX)^{-1}X^TY = \frac{(X^TX)^*}{|X^TX|}X^TY
$$

- 若自变量存在完全线性相关, 如, $X_1 = 2 X_2$, 则矩阵$X^TX$的行列式$|X^TX|$为0, 矩阵$X^TX$不可逆, 不存在逆矩阵$(X^TX)^{-1}$, 此时求不出$\beta$的最小二乘估计.
- 自变量存在高度相关关系时, 如, $X_1 \approx 2X_2$, 则矩阵$X^TX$的行列式$|X^TX|$近似为0, 由于$|X^TX|$处在坟墓上, 此时回归系数$b$的偏差会很大, 也即$\beta$的最小二乘估计量会很大.

## 直观解释

$$
b = (X^TX)^{-1}X^TY
$$

假设我们只有两个independent variable，它们对应的column vector如下图$X_1$和$X_2$, Regression prediction便是投影 $y'$。

![](./4多重共线性(multicollinearity)/1.png)

当$X_1$和$X_2$呈线性关系(或接近线性关系), 它们就会重合成一条直线, 有无限个平面穿越他们, 相应的投影也不确定.

![](./4多重共线性(multicollinearity)/2.png)

## 具体案例
如果有:

$$
y=x_1+x_2 \\
x_1=2x_2
$$

那么:
$$
y = 1.0x_1 + 1.0x_2 \\
y = 0.0x_1 + 3.0x_2 \\
y = 1.5x_1 + 0.0x_2 \\
y = 0.5x_1 + 2.0x_2 \\
y = 0.1x_1 + 2.8x_2
$$

就是说当$x_1$和$x_2$之间存在完全线性相关关系时, 会导致不同的$x_1$和$x_2$组合形式都能得到相同的$y$, 那么$y$和$x_1, x_2$就是是何种数量关系就让人难以捉摸了.

## 方差膨胀因子
通过方差膨胀因子(Variance inflation factor)和容忍度(tolerance)来诊断多重共线性,VIF和容忍度两者互为倒数。

每个自变量都有其VIF和容忍度, 对于自变量$X_i$, 以$X_i$为因变量, 以$X_i$外的各自变量为自变量建立线性回归模型, 回归模型的决定系数记为$R_i^2$. 则自变量$X_i$的容忍度$tolerance = 1 - R_i^2$, 方差的膨胀因子$VIF=\frac{1}{1-R_i^2}$.

一般, 当$VIF$的最大值$>10$, 或者$VIF$的平均值大于1的时候, 可认为存在共线性


## 多重共线性问题的处理

- 将一个或多个相关的自变量从模型中剔除，使保留的自变量尽可能不相关。
- 如果要在模型中保留所有的自变量，就应该：
    - 避免根据$t$统计量对单个参数$\beta$进行检验。
    - 对因变量$y$值的推断（估计或预测）限定在自变量样本值的范围内。


多重共线性问题带来的主要麻烦是对单个回归系数的解释和检验。在求因变量的置信区间和预测区间时一般不会受其影响，但必须保证用于估计或预测的自变量的值是在样本数据的范围之内。因此，`如果仅仅是为了估计或预测，可以将所有自变量都保留在模型中`。


最后需要提醒的是：在建立多元线性回归模型时，不要试图引入更多的自变量，除非确实有必要。特别是在社会科学的研究中，由于所使用的大多数数据都是非试验性质的，因此，在某些情况下，得到的结果往往并不令人满意，但这不一定是因为选择的模型不合适，而是数据的质量不好，或者是引入的自变量不合适。


## 参考
- 统计学第8版253页
- [一看就懂的多重共线性](https://zhuanlan.zhihu.com/p/355241680)
- [通俗地理解Multicollinearity](https://zhuanlan.zhihu.com/p/444280602)








