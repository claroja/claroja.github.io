# 变量选择与逐步回归

## 变量选择过程

究竟哪些自变量应该引入模型，哪些自变量不应该引入模型，需要对自变量进行一定的筛选。在进行回归时，每次只增加一个变量，并且将新变量与模型中的变量进行比较，若新变量引入模型后以前的某个变量的统计量不显著，这个变量就会被从模型中剔除，在这种情况下，回归分析就很难受到多重共线性的影响，这就是回归中的搜寻过程。逐步回归是避免多重共线性的方法之一。

选择自变量的原则通常是对统计量进行显著性检验，检验的根据是：将一个或一个以上的自变量引入回归模型中时，是否使残差平方和(SSE)显著减少。如果增加一个自变量使残差平方和显著减少，则说明有必要将这个自变量引入回归模型，否则，就没有必要将这个自变量引入回归模型。确定在模型中引入自变量$x_i$是否使残差平方和显著减少的方法，就是使用$F$统计量的值作为一个标准，以此来确定是在模型中增加一个自变量，还是从模型中剔除一个自变量。变量选择的方法主要有向前选择(forward selection)、向后剔除(backward elimina-tion)、逐步回归(stepwise regression)


## 向前选择(forward selection)
向前选择法是从模型中没有自变量开始，然后按下面的步骤选择自变量来拟合模型。

第1步：对$k$个自变量$(x_1,x_2,...,x_k)$分别拟合与因变量$y$的一元线性回归模型，共有$k$个，然后找出$F$统计量的值最大的模型及其自变量$x_i$,并将其首先引入模型。(如果所有模型均无统计上的显著性，则运算过程终止，没有模型被拟合。)

第2步：在已经引入模型的$x_i$的基础上，再分别拟合$k-1$个自变量$(x_1,...,x_{i-1},x_{i+1},...,x_k$的线性回归模型即变量组合为$x_i+x_1,...,x_i+x_{i-1},x_i+x_{i+1},...,x_i+x_k$的$k-1$个线性回归模型。然后分别考察这$k-1$个线性模型，挑选出$F$统计量的值最大的含有两个自变量的模型，并将$F$统计量的值最大的那个自变量$x_j$,引入模型。如果除$x_i$之外的$k-1$个自变量中没有一个是统计上显著的，则运算过程终止。如此反复进行，直至模型外的自变量均无统计上的显著性为止。

向前选择变量的方法是不停地向模型中增加自变量，直至增加自变量不能导致SSE显著增加(这个过程通过F检验来完成)为止。由此可见，只要将某个自变量增加到模型中，这个变量就一定会保留在模型中。


总结步骤:
1. 以空模型开始, 不包含任何变量
2. 逐步增加罪显著的变量
3. 直到到达预设的规则或者将所有的变量都添加进去


关键点在于:

- 如何确定最显著的变量
    - 有最小的p-value
    - 提供最大的$R^2$增加
    - 最大化减少RSS

- 如何选择停止的规则
    停止规则是指在剩余的的变量中, 如果添加一个就能使p-value大于阈值. 如果触发这个规则就会停止, 然后返回一个仅满足p-values < threshold的包含的所有变量.
    
    如果是固定值, 则阈值对于所有变量都是一样的, 而AIC和BIC则每个变量都不同. BIC比AIC更严格, 所以建议在变量的样本量超过100时才使用.

    - 一个固定的值, 如0.05 or 0.2 or 0.5
    - AIC (Akaike Information Criterion)
    
        AIC根据变量的自由度来选择, 如, 一个二进制变量(variable), 有1个自由度(degree of freedom), 如果该变量被包含在模型中, 需要p-value < 0.157. 越大的自由度, 对应越小的p-value.
    
    - BIC (Bayesian information criterion)

        BIC根据变量的有效样本量. 如, 样本量为20, 该变量需要 p-value < 0.083, 才能加入模型. 样本量越大, 对应p-value越小.


## 向后剔除(backward elimina-tion)
与向前选择法相反，向后剔除法的基本过程如下：

第1步：先对因变量拟合包括所有$k$个自变量的线性回归模型。然后考察$p(p<k)$个去掉一个自变量的模型(这些模型中的每一个都有$k-1$个自变量)，使模型的SSE值减小最少的自变量被挑选出来并从模型中剔除。

第2步：考察$p一1$个再去掉一个自变量的模型（这些模型中的每一个都有$k-2$个自变量），使模型的SSE值减小最少的自变量被挑选出来并从模型中剔除。如此反复进行，一直将自变量从模型中剔除，直至剔除一个自变量不会使SSE显著减小为止。这时，模型中所剩的自变量都是显著的。上述过程可以通过$F$检验的$P$值来判断。


总结步骤:

1. 将所有变量都加入模型
2. 移除最小显著性的变量
3. 直到到达预设的规则或者将所有的变量都移除

关键点在于:

- 如何确定最不显著的变量
    - 有最大的p-value
    - 提供最小的$R^2$减少
    - 最小化减少RSS

- 如何选择停止的规则

    停止规则是当模型中剩余的变量, 使得p-value小于预设的值. 当达到这个阈值时, 将停止, 并返回包含现有变量的模型.

    - 一个固定的值, 如0.05 or 0.2 or 0.5
    - AIC (Akaike Information Criterion)
    - BIC (Bayesian information criterion)


## 逐步回归(stepwise regression)

逐步回归是将上述两种方法结合起来筛选自变量的方法。前两步与向前选择法相同。不过在增加了一个自变量后，它会对模型中所有的变量进行考察，看看有没有可能剔除某个自变量。如果在增加了一个自变量后，前面增加的某个自变量对模型的贡献变得不显著，这个变量就会被剔除。因此，逐步回归是向前选择和向后剔除的结合。逐步回归过程就是按此方法不停地增加变量并考虑剔除以前增加的变量的可能性，直至增加变量不会导致SSE显著减少，这个过程可通过F统计量来检验。逐步回归法中，在前面步骤中增加的自变量在后面的步骤中有可能被剔除，而在前面步骤中被剔除的自变量在后面的步骤中也可能重新进入模型。





## 参考:

- [Understand Forward and Backward Stepwise Regression](https://quantifyinghealth.com/stepwise-selection/)- 
- [stepwise-regression](https://dataaspirant.com/stepwise-regression/)
- [Linear regression](https://dataaspirant.com/linear-regression-implementation-in-python/) 
- [Logistic regression](https://dataaspirant.com/- implement-logistic-regression-model-python-binary-classification/)
- [Polynomial regression](https://dataaspirant.com/polynomial-regression/)
- [Ridge regression](https://dataaspirant.com/ridge-regression/)
- [Lasso regression](https://dataaspirant.com/lasso-regression/)
