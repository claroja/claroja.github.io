# 回归模型评估

如果各观测数据的散点都落在一条直线上，那么这条直线就是对数据的完全拟合，直线充分代表了各个点，此时用x来估计y是没有误差的。各观测点越是紧密围绕直线，说明直线对观测数据的拟合程度越好，反之则越差。回归直线与各观测点的接近程度称为回归直线对数据的拟合优度(goodness of fit). 为说明直线的拟合优度，需要计算判定系数。

## 判定系数

因变量$y$的取值是不同的，$y$取值的这种波动称为变差。变差的产生来自两个方面：一是由自变量$x$的取值不同造成的；二是除$x$以外的其他因素（如$x$对$y$的非线性影响、测量误差等）的影响。对一个具体的观测值来说，变差的大小可以用实际观测值$y$与其均值$\overline{y}$之差$y-\overline{y}$来表示。而$n$次观测值的总变差可由这些离差的平方和来表示，称为总平方和，记为SST, 即
$$
SST = \sum(y_i - \overline{y})^2
$$

每个观测点的离差都可以分解为:

$$
y-\overline{y} = (y-\hat{y}) + (\hat{y}-\overline{y})
$$

将两边平方，并对所有$n$个点求和，有
$$
\sum(y_i - \overline{y})^2 = \sum(y_i - \hat{y}_i)^2 + \sum(\hat{y}_i - \overline{y})^2 + 2\sum(y_i-\hat{y}_i)(\hat{y}_i-\overline{y})
$$

可以证明, $\sum(y_i-\hat{y}_i)(\hat{y}_i-\overline{y})=0$, 因此

$$
\sum(y_i - \overline{y})^2 = \sum(y_i - \hat{y}_i)^2 + \sum(\hat{y}_i - \overline{y})^2
$$

左边称为总平方和SST(Sum of Squares Total),它可分解为两部分：
- 残差平方和或误差平方和(SSE, Sum of Squares Error): $\sum(y_i - \hat{y}_i)^2$是各实际观测点与回归值的残差$y_i - \hat{y}_i$平方和，它是除了$x$对$y$的线性影响之外的其他因素引起的y的变化部分，是不能由回归直线来解释的$y_i$的变差部分。

- 回归平方和(SSR, Sum of Squares Regression): $\sum(\hat{y}_i - \overline{y})^2$是回归值$\hat{y}_i$与均值$\overline{y}$的离差平方和, 根据估计的回归方程, 估计值$\hat{y}_i = \hat{\beta_0} + \hat{\beta_1} x_i$, 因此可以把$\hat{y}_i - \overline{y}$看做由于自变量$x$的变化引起的$y$的变化, 而其平方和$\sum(\hat{y}_i - \overline{y})^2$则反映了$y$的总变差中由于$x$与$y$之间的线性关系引起的$y$的变化部分, 它是可以由回归直线来解释的$y_i$的变差部分



三个平方和的关系为：

$$
总平方和(SST)=残差平方和(SSE) + 回归平方和(SSR)
$$

回归直线拟合的好坏取决于SSR及SSE的大小，或者说取决于回归平方和SSR占总平方和SST的比例(SSR/SST)的大小。各观测点越是靠近直线，SSR/SST则越大，直线拟合得越好。回归平方和占总平方和的比例称为判定系数(coefficient of determination),记为$R^2$,其计算公式为：

$$
R^2 = \frac{SSR}{SST} = \frac{\sum(\hat{y}_i - \overline{y})^2}{\sum(y_i - \overline{y})^2} = 1- \frac{\sum(y_i - \hat{y}_i)^2}{\sum(y_i - \overline{y})^2}
$$

判定系数$R^2$测度了回归直线对观测数据的拟合程度。若所有观测点都落在直线上，
残差平方和SSE=0,则$R^2=1$,拟合是完全的；如果$y$的变化与$x$无关，$x$完全无助于

解释$y$的变差，$\hat{y}= \overline{y}$，则$R^2=0$。可见$R^2$的取值范围是`[0,1]`。$R^2$越接近1，表明回归平方和占总平方和的比例越大，回归直线与各观测点越接近，用$x$的变化来解释$y$值变差的部分就越多，回归直线的拟合程度就越好；反之，$R^2$越接近0，回归直线的拟合程度就越差。


在一元线性回归中，相关系数$r$实际上是判定系数的平方根。根据这一结论，不仅可以由相关系数直接计算判定系数$R^2$,也可以进一步理解相关系数的意义。相关系数$r$与回归系数$\hat{\beta}_1$的正负号是相同的，实际上，相关系数$r$从另一个角度说明了回归直线的拟合优度。$|r|$越接近1，表明回归直线对观测数据的拟合程度越好。但用$r$说明回归直线的拟合优度要慎重，因为$r$的值总是大于$R^2$的值(除非$r=0$或$|r|=1$)。比如，当$r=0.5$时，表面上看相关程度似乎接近一半，但$R^2=0.25$,实际上这只能解释总变差的25%。$r=0.7$才能解释近一半的变差，$r<0.3$意味着只有很少一部分变差可由回归直线来解释。




## 估计标准误差

残差平方和则可以说明实际观测值$y_i$与回归估计值$\hat{y}_i$之间的差异程度。估计标准误差(standard error of estimate)就是度量各实际观测点在直线周围的散布状况的一个统计量，它是均方残差(MSE)的平方根，用$s_e$。来表示，其计算公式为：
$$
s_e = \sqrt{\frac{\sum (y_i - \hat{y}_i)^2}{n-2}} = \sqrt{\frac{SSE}{n-2}}=\sqrt{MSE}
$$

估计标准误差是对误差项$\epsilon$的标准差$\sigma$的估计，它可以看作在排除了$x$对$y$的线性影响后，$y$随机波动大小的一个估计量。从估计标准误差的实际意义看，它反映了用估计的回归方程预测因变量$y$时预测误差的大小。各观测点越靠近直线，$s_e$越小，回归直线对各观测点的代表性就越好，根据估计的回归方程进行预测也就越准确。若各观测点全部落在直线上，则$s_e=0$，此时用自变量来预测因变量是没有误差的。可见$s_e$从另一个角度说明了回归直线的拟合优度。

回归直线是对$n$个观测点拟合的所有直线中，估计标准误差最小的一条直线，因为回归直线是当$\sum (y_i - \hat{y}_i)^2$最小时确定的。

## 参考
- 统计学第8版227页