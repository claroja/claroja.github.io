

# 线性关系和回归系数检验
建立了估计方程后，还不能马上进行估计或预测，因为该估计方程是根据样本数据得出的，它是否真实地反映了变量x和y之间的关系，需要通过检验来证实。

根据样本数据拟合回归方程时，实际上已经假定变量x与y之间存在线性关系，即$y=\beta_0 + \beta_1x + \epsilon$,并假定误差项$\epsilon$是一个服从正态分布的随机变量，且对不同的$x$具有相同的方差。但这些假设是否成立，需要通过检验来证实。


## 线性关系的检验

线性关系检验是检验自变量$x$和因变量$y$之间的线性关系是否显著，或者说，它们之间能否用一个线性模型$y=\beta_0 + \beta_1x + \epsilon$来表示。

为检验两个变量之间的线性关系是否显著，需要构造用于检验的统计量。该统计量的构造是以回归平方和(SSR)和残差平方和(SSE)为基础的。
- 将SSR除以其相应的自由度(SSR的自由度是自变量的个数$k$,一元线性回归中自由度为1)的结果称为均方回归，记为MSR;
- 将SSE除以其相应的自由度(SSE的自由度为$n-k-1$，一元线性回归中自由度为$n-2$)的结果称为均方残差，记为MSE。

如果原假设成立($H_0: \beta_1=0,两个变量之间的线性关系不显著$)，则比值MSR/MSE的抽样分布服从分子自由度为1、分母自由度为$n-2$的$F$分布，即

$$
F = \frac{SSR/1}{SSE/(n-2)} = \frac{MSR}{MSE} \sim F(1, n-2)
$$

所以当原假设$H_0: \beta_1=0$成立时，MSR/MSE的值应接近1，但如果原假设$H_0:\beta_1=0$不成立，MSR/MSE的值将变得无穷大。因此，较大的MSR/MSE值将导致拒绝原假设$H_0$，此时就可以断定变量$x$与$y$之间存在显著的线性关系。线性关系检验的具体步骤如下。


1. 第1步：提出假设。
$$
H_0: \beta_1=0, 两个变量之间的线性关系不显著
$$


2. 第2步：计算检验统计量F。
$$
F = \frac{SSR/1}{SSE/(n-2)} = \frac{MSR}{MSE}
$$

3. 第3步：作出决策。确定显著性水平$\alpha$,并根据分子自由度$df_1=1$和分母自由度$df_2=n-2$查$F$分布表，找到相应的临界值$F_\alpha$。
    - 若$F>F_\alpha$，拒绝$H_0$,表明两个变量之间的线性关系是显著的；
    - 若$F<F_\alpha$，不拒绝$H_0$，没有证据表明两个变量之间的线性关系显著。



## 回归系数的检验
回归系数的显著性检验是要检验自变量对因变量的影响是否显著。在一元线性回归模型$y=\beta_0 + \beta_1x + \epsilon$，如果回归系数$\beta_1=0$,则回归线是一条水平线，表明因变量$y$的取值不依赖于自变量$x$,即两个变量之间没有线性关系。如果回归系数$\beta_1 \neq 0$，也不能得出两个变量之间存在线性关系的结论，要看这种关系是否具有统计意义上的显著性。回归系数的显著性检验就是检验回归系数$\beta_1$是否等于0。为检验原假设$H_0:\beta_1=0$是否成立，需要构造用于检验的统计量。为此，需要研究回归系数$\beta_1$的抽样分布。

估计的回归方程$\hat{y}_i=\hat{\beta}_0 + \hat{\beta}_1x_i$是根据样本数据计算的。当抽取不同的样本时，就会得出不同的估计方程。实际上，$\hat{\beta}_0$和$\hat{\beta}_1$是根据最小二乘法得到的用于估计参数$\beta_0$和$\beta_1$的统计量，它们都是随机变量，都有自己的分布。根据检验的需要，这里只讨论$\hat{\beta}_1$的分布。统计证明，$\hat{\beta}_1$服从正态分布，其数学期望为$E(\hat{\beta}_1)=\beta_1$,标准差为：
$$
\sigma_{\beta_1}=\frac{\sigma}{\sqrt{\sum x_i^2 - \frac{1}{n}(\sum x_i)^2}}
$$

其中, $\sigma$是误差项$\epsilon$的标准差.

由于$\sigma$未知，将$\sigma$的估计量$s_e$代人得到$\sigma_{\beta_1}$的估计量，即$\hat{\beta_1}$的估计的标准差为：
$$
S_{\hat{\beta}_1} = \frac{S_e}{\sqrt{\sum x_i^2 - \frac{1}{n}(\sum x_i)^2}}
$$

这样就可以构造出用于检验回归系数$\beta_1$的统计量$t$:
$$
t = \frac{\hat{\beta}_1-\beta_1}{S_{\hat{\beta}_1}}
$$

该统计量服从自由度为$n-2$的$t$分布。如果原假设成立，则$\beta_1=0$，检验的统计量为：
$$
t = \frac{\hat{\beta}_1}{S_{\hat{\beta}_1}}
$$

回归系数的显著性检验的具体步骤如下：

1. 第1步: 提出检验
$$
H_0: \beta_1 = 0 ; H_1: \beta_1 \neq 0
$$

2. 第2步: 计算检验统计量
$$
t = \frac{\hat{\beta}_1}{S_{\hat{\beta}_1}}
$$

3. 第3步：作出决策。确定显著性水平$\alpha$,并根据自由度$df=n-2$查$t$分布表，找到相应的临界值$t_{\alpha/2}$。
    - 若$|t|>t_{\alpha/2}$,则拒绝$H_0$,回归系数等于0的可能性小于$\alpha$,表明自变量$x$对因变量$y$的影响是显著的，换言之，两个变量之间存在显著的线性关系；
    - 若$|t|<t_{\alpha/2}$,则不拒绝$H_0$,没有证据表明$x$对$y$的影响显著，或者说，二者之间尚不存在显著的线性关系。


## 参考
- 统计学第8版230页