# 最小化损失

## 残差
残差是响应变量的实际值和通过模型预测的值之间的差, 英文为residual. 以啤酒销售为例:
$$
啤酒销售额 = N(\beta_0 + \beta_1 \times 气温, \sigma^2)
$$
如果气温为20度, 啤酒销售额的期望值就是$\beta_0+\beta_1 \times 20$. 这就是"当气温为20度时啤酒销售额的预测值(点估计值)".
设响应变量(啤酒的销售额)为$y$, 模型预测值的响应变量的预测值为$\hat{y}$, 则残差如下:
$$
residual = y - \hat{y}
$$

## 残差平方和
平方即两个数相乘, 计算残差的平方并求和, 得到的就是残差平方和. 容量为n的样本的残差平方和的计算公式如下:
$$
残差平方和 = \sum_{i=1}^n(y_i-\hat{y}_i)^2
$$


## 最小二乘法
求使得残差平方和最小的参数, 并把这个参数作为参数估计值的方法就是最小二乘法. 另一种说法是, 最小二乘法是以残差平方和为损失目标, 求得损失最小的参数的方法. 普通最小二乘法的英文为 Ordinary Least Squares, 通常写为: OLS.

## 最小二乘法与最大似然法的关系
最小二乘法得到的参数估计值等于假设总体服从正态分布时最大似然法的结果.
最大似然法令对数似然最大:
$$
\begin{align*}
    &\argmax_{\beta_0,\beta_1}\ln\zeta  \\
    &=\argmax_{\beta_0,\beta_1}\sum_{i=1}^n(ln(\frac{1}{\sqrt{2\pi\sigma^2}})-\frac{(y_i-(\beta_0+\beta_1x_i))^2}{2\sigma^2})
\end{align*}
$$
其中, $\sigma^2$为多余参数, 无需直接估计, 这样就可以忽略$1/\sqrt{2\pi\sigma^2}$和$2\sigma^2$. 将预测值带入$\hat{y}_i = \beta_0 + \beta_1$, 得到:
$$
\begin{align*}
    &\argmax_{\beta_0,\beta_1}\ln\zeta  \\
    &=\argmax_{\beta_0,\beta_1}\sum_{i=1}^n-(y_i - \hat{y}_i)^2
\end{align*}
$$

要让$\sum_{i=1}^n-(y_i - \hat{y}_i)^2$最大, 只要去掉负号即可, 这就相当于让残差平方和最小. 所以最小二乘法的估计值等于假设总体服从正态分布时最大似然法的估计值.
最小二乘法在实践中是一种非常高效的方法, 所以为了简化流程, 我们应该在允许的情况下使用最小二乘法.


