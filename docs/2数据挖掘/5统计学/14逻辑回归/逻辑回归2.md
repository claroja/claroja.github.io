# 逻辑回归2

逻辑回归`逻辑`是音译`logistic`的缩写.

机器学习算法中的监督式学习可以分为2大类：
分类模型：目标变量是分类变量（离散值）；
回归模型：目标变量是连续性数值变量。

逻辑回归通常用于解决分类问题, “分类”是应用逻辑回归的目的和结果，但中间过程依旧是“回归”。因为通过逻辑回归模型，我们得到的计算结果是0-1之间的连续数字，可以把它称为“可能性”（概率）。对于上述问题，就是：客户购买某个商品的可能性，借款人违约的可能性。

然后，给这个可能性加一个阈值，就成了分类。例如，算出贷款违约的可能性>0.5，将借款人预判为坏客户。


## 多元线性回归
多元线性回归方程的一般形式为：

$$
y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + ... + \beta_p x_p
$$

其中: $x$表示自变量, $y$表示因变量, $x$和$\beta$的下标$p$表示自变量的个数, 即维度.

矩阵形式为:

$$
Y = X \beta
$$

其中:

$$
Y = 
\begin{bmatrix} 
y_1 \\
y_2 \\ 
... \\
y_n
\end{bmatrix}
,
X = 
\begin{bmatrix} 
1 & x_{11} & ... & x_{1p} \\ 
2 & x_{21} & ... & x_{2p} \\ 
... & ... & ... & ...\\
n & x_{n1} & ... & x_{np}
\end{bmatrix}
,
\beta = 
\begin{bmatrix} 
\beta_0 \\
\beta_1 \\ 
... \\
\beta_p
\end{bmatrix}

$$

将$X\beta$带入sigmoid函数$f(z) = \frac{1}{1+e^{-z}}$中的$z$, 得到$\frac{1}{1+e^{-X \beta}}$, 令其为预测为正例的概率$P(Y=1)$，那么逻辑回归的形式就有了：
$$
P(Y=1) = \frac{1}{1+e^{-X \beta}}

$$

到目前为止，逻辑函数的构造算是完成了。找到了合适的函数，下面就是求函数中的未知参数向量$β$了。求解之前，我们需要先理解一个概念——似然性。

## 似然函数

我们常常用概率(Probability) 来描述一个事件发生的可能性。而似然性(Likelihood) 正好反过来，意思是一个事件实际已经发生了，反推在什么参数条件下，这个事件发生的概率最大。

用数学公式来表达上述意思，就是:
- 已知参数$β$前提下，预测某事件$x$发生的条件概率为$P(x|\beta)$;
- 已知某个已发生的事件$x$，未知参数$β$的似然函数为$\zeta(\beta|x)$；
- 上面两个值相等，即$\zeta(\beta|x)=P(x|\beta)$。

一个参数$β$对应一个似然函数的值，当$β$发生变化，$\zeta(\beta|x)$也会随之变化。当我们在取得某个参数的时候，似然函数的值到达了最大值，说明在这个参数下最有可能发生x事件，即这个参数最合理。

因此，最优$β$，就是使当前观察到的数据出现的可能性最大的$β$。


## 最大似然估计
在二分类问题中，$y$只取0或1，可以组合起来表示y的概率:
$$
P(y) = P(y=1)^yP(y=0)^{1-y}
$$

我们可以把$y=1$代入上式验证下：

- 左边是: $P(y=1)$
- 右边是: $P(y=1)^1P(y=0)^0 = P(y=1)$

更严谨的写法需要加上特征$x$和参数$β$：

$$
P(y|x,\beta) = P(y=1|x,\beta)^y(1-P(y=1|x,\beta))^{1-y}
$$


前面说了，$\frac{1}{1+e^{-X \beta}}$表示的就是P(y=1)，代入上式：

$$
P(y|x,\beta) = (\frac{1}{1+e^{-X\beta}})^y(1- \frac{1}{1+e^{-X \beta}})^{1-y}
$$


根据上一小节说的最优$β$的定义，也就是最大化我们见到的样本数据的概率，即求下式的最大值。


$$
\zeta(\beta) = \prod_{i=1}^n P(y_i|x_i, \beta) = \prod_{i=1}^n(\frac{1}{1+e^{-x_i \beta}})^{y_i}(1- \frac{1}{1+e^{-x_i \beta}})^{1-y_i}
$$

$\zeta(\beta|x)=P(x|\beta)$, 对于某个观测值$y_i$, 似然函数的值$\zeta(\beta|y_i)$, 就等于条件概率的值$P(y_i|\beta)$

因为一系列的xi和yi都是我们实际观测到的数据，式子中未知的只有$β$。因此，现在问题就变成了求$β$在取什么值的时候，$L(β)$能达到最大值。

$L(β)$是所有观测到的y发生概率的乘积，这种情况求最大值比较麻烦，一般我们会先取对数，将乘积转化成加法。

$$
log \zeta(\beta) = \sum_{i=1}^n(y_i \cdot log(\frac{1}{1+e^{-x_i \beta}})+ (1-y_i) \cdot log(1 - \frac{1}{1 + e^{-x_i \beta}}))
$$

## 损失函数
损失函数是用于衡量预测值与实际值的偏离程度，即模型预测的错误程度。也就是说，这个值越小，认为模型效果越好.

在线性回归一文中，我们用到的损失函数是残差平方和SSE：
$$
Q = \sum_1^n(y_i - \hat{y}_i)^2 = \sum_1^n(y_i - x_i \beta)^2
$$

这是个凸函数，有全局最优解。

如果逻辑回归也用平方损失，那么就是：
$$
Q = \sum_1^n(y_i - \frac{1}{1+e^{-x_i \beta}})^2
$$

这个不是凸函数，不易优化，容易陷入局部最小值，所以逻辑函数用的是别的形式的函数作为损失函数，叫对数损失函数（log loss function）。

这个对数损失，就是上一小节的对数似然函数的相反数：

$$
J(\beta) = - log \zeta(\beta) = - \sum_{i=1}^n (y_i \cdot log(\frac{1}{1+e^{-x_i \beta}})+ (1-y_i) \cdot log(1 - \frac{1}{1 + e^{-x_i \beta}}))
$$




## 参考
- https://zhuanlan.zhihu.com/p/139122386
- https://blog.csdn.net/csdn_lyy/article/details/117399423