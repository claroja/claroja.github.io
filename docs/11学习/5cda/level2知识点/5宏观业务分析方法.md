# 第五章宏观业务分析方法

## 矩阵分析法
矩阵分析法就是大数据小分析, 在做决策时对数据进行降维, 选取一些`有代表性的变量`进行分析.

企业可以获取的产品信息有很多, 比如利润率, 费用比率, 年销售增长率, 市场饱和度, 产品知名度和市场占有率等. 波士顿咨询公司认为, 只有`市场占有率`和`市场成长率`两个变量最具价值, 这就是`波士顿矩阵`. `产品市场占有率`和`利润率`, `产品知名度`有较强的关系, 反映了该产品的市场地位和产生现金流的能力; 而`市场成长率`和`市场饱和度强`相关, 反映了产品的市场发展潜力, 也就是说两个变量是`最具代表性的`.

`产品的生命周期`包含: `初创期`, `成长期`, `成熟期`和`衰退期`.
1. 初创期: 产品的收益率存在较大的波动性, 且成长率不太高
2. 成长期: 销售额及盈利均呈现快速增长的趋势
3. 成熟期: 市场增长率开始下降, 收益率提高
4. 衰退期: 市场已经有很多衰退期产品, 竞争非常激烈, 市场增长率也会偏低

`市场成长率`反映了`产品生命周期`的变化情况, 而`市场占有率`反映了企业具体产品的`市场渗透情况`, 二者`相互独立`(市场成长率反映市场情况, 市场占有率反映企业在市场上的状况). 以IBM公司对个人笔记本, 服务器, 咨询这几种不同类型的业务做出决策为例. 
1. PC业务, 市场占有率并不高, 同时市场成长率也较低, 属于瘦狗产品
2. 服务器业务, 市场占有率非常高, 市场成长率处于成长期和成熟期之间, 偏向于金牛产品
3. 咨询业务, 市场占有率偏低, 而市场成长率很高, 属于`问号产品`
IBM最后的决策是:
1. 将瘦狗产品PC业务进行出售
2. 金牛产品服务器业务继续保留, 并继续投资
3. 问号产品咨询业务经过不断扶持, 发展成明星产品, 之后称为金牛产品

对于瘦狗产品, 一般不会对其进行发展性资源的分配, 而且制定的绩效会比较苛刻
对于问号产品, 要观察未来市场发展的情况, 如果发展良好, 市占率不断增加, 则配置战略性资源, 制定温和的绩效目标(比如用户粘性, 用户增长略, 用户好评度等)
对于金牛产品, 更加关注收益类指标, 之于用户粘性则不做过多关注


## 连续变量降维
建模的两大问题是引入了`不相关变量`和`冗余变量`。

1. 不相关变量的处理方法：

    1. 在分类算法中可以通过两个变量的统计检验，逐步法， 决策树等剔除不相关变量；
    2. 而聚类模型完全依靠分析师的个人经验。

2. 冗余变量的处理方法：统一采用主成分分析法, 及其变体. 将相关变量合成一个变量或只选择其中一个变量


特征选择和转换理论可以针对所有类型的变量, 但在实操中只有连续变量才能比较方便的降维, 分类变量往往先进行水平规约, 然后转换为连续变量(WoE转换).


1. `变量筛选(特征选择, 针对不相关变量)`: 从一些相关性较强的变量中提取`有代表性的变量`, 保存对被解释变量有解释价值的变量. 变量筛选的方法分为两个变量`独立性校验`和`模型筛选法`.
    1. `独立性校验`: 通过被解释变量和单个变量之间做假设检验完成, 如`卡方检验`, `方差分析(两个样本t检验)`, 相关分析.
    2. `模型筛选法`: 模型算法自带的, 如, `决策树通过计算熵增益, 基尼增益等指标筛选出高价值变量`.
2. `维度规约(特征转换, 针对冗余变量)`:是指将相关性较强的一组变量`合并成一个变量`. 预测类模型要求信息损失越小越好, 而聚类模型为了追求模型的可解释性, 信息损失可以多一点
    1. 主成分分析: 考察多个变量之间线性相关的一种多元统计方法. 从原始变量中导出几个主成分, 使他们尽可能多的保留原始变量的信息, 且彼此之前互不相关, 新的主成分不再具有可解释性. 该方法多用于不可解释类型的预测模型, 如, 神经网络, 支持向量机.
    2. 因子分析: 因子旋转法的因子分析是主成分分析的延伸, 通过调整主成分在原始变量中的权重来发现主成分代表的含义. 常用于, 描述性统计分析和聚类模型, 多用于市场研究, 研究报告中, 不用于预测类模型建模.
    3. 奇异值分解: 是主成分分析在非方阵情况下的推广. 常用于, 推荐算法和缺失值填补.

## 主成分分析
主成分分析(Principal components analysis，PCA)的目的是构造`输入变量的线性组合`, 从而达到`降维的目的`, 并且`尽可能多的解释原始数据的变异性`. `这些线性组合`被称为`主成分`.

主成分是在原始数据空间中的线性变换, 其表达式如下:

$$
P_1 = \beta_1 * X = \beta_{11}X_1 + \beta_{12}X_2 + ... + \beta_{1p}X_p \\
P_i = \beta_i * X = \beta_{i1}X_1 + \beta_{i2}X_2 + ... + \beta_{ip}X_p \\
P_p = \beta_p * X = \beta_{p1}X_1 + \beta_{p2}X_2 + ... + \beta_{pp}X_p \\
$$

设多个随机变量集合为 $X = (X_1, X_2, X_3, ... , X_p)$ , 其协方差矩阵为:

$$
\sum = 
\begin{bmatrix}
cov(X_1, X_1) & cov(X_1,X_2) & ... & cov(X_1, X_p) \\ 
cov(X_2, X_1) & cov(X_2,X_2) & ... & cov(X_2, X_p) \\
cov(X_p, X_1) & cov(X_p,X_2) & ... & cov(X_p, X_p)
\end{bmatrix}
$$

通过线性代数知识可以得到主成分$P$的方差计算公式:
$$
VAR(P) = \beta \sum \beta^{T}
$$

其中, $\beta$ 代表`主成分`在`每个变量上的权重`. 主成分算法的目标函数是计算`特征向量` $\beta$ , 使得`主成分上的方差取得最大值`, 这个求极值的过程就是线性代数中计算方阵的`特征向量(主成分)`和`特征根(主成分的方差)`的算法.



### 原理:
1. 主成分的个数等于原始变量的个数
2. 第一个主成分是`方差最大`的投影, 剩余的主成分依次与之前的主成分`正交`
3. 主成分的`变异(方差)之和`等于`原始变量的所有变异(方差)之和`
4. 单个主成分的变异(特征值)不应该小于1,
5. 前几个主成分的变异可以解释原始变量绝大部分的变异, 够达到总体的80%~90%. 主成分解释变异的能力可以以方差比例计算
    $$
    \frac{\lambda_k}{\lambda_{1}+...+\lambda_{n}}
    $$
6. 如果原始变量不相关, 即`协方差为0`, 则`不需要做主成分分析`
7. 如果变量的尺度(单位)不一样, 造成范围不相当, 可以`先进行归一化处理`, 然后`再计算协方差矩阵`. 或者`直接计算相关系数矩阵`.
8. 可以根据每个主成分对应的主成分方程来解释主成分的含义:
    $$
    P_1 = \beta_{11}X_{1} + \beta_{12}X_{2} + \beta_{1n}X_{n}
    $$

    若 $\beta_{12}$ 最大, 则 $X_{2}$ 所占权重最大, 该主成分就可以使用变量 $X_{2}$ 的实际含义来解释.

### 主成分分析的应用
1. 综合打分: 如给员工绩效打分, 对于单向成绩简单加总的方法, 主成分分析会使评分更聚焦于单一维度上, 即更关注原始变量的共同部分, 去掉不相关的部分. 但如果不支持取一个主成分时, 就不能使用该方法了
2. 数据描述: 如波士顿矩阵
3. 为聚类回归等压缩变量
4. 去除噪声: 如图像识别


### 代码

```python
import pandas as pd
#1.导入数据
model_data = pd.read_csv("cities_10.csv",encoding='gbk')
model_data.head()

```

AREA|X1|X2|X3|X4|X5|X6|X7|X8|X9
--|--|--|--|--|--|--|--|--|--
辽宁|5458.2|13000|1376.2|2258.4|1315.9|529.0|2258.4|123.7|399.7
山东|10550.0|11643|3502.5|3851.0|2288.7|1070.7|3181.9|211.1|610.2
河北|6076.6|9047|1406.7|2092.6|1161.6|597.1|1968.3|45.9|302.3
天津|2022.6|22068|822.8|960.0|703.7|361.9|941.4|115.7|171.8
江苏|10636.3|14397|3536.3|3967.2|2320.0|1141.3|3215.8|384.7|643.7

字段含义:
X1|GDP
X2|人均GDP
X3|工业增加值
X4|第三产业增加值
X5|固定资产投资
X6|基本建设投资
X7|社会消费品零售总额
X8|海关出口总额
X9|地方财政收入


```python
## 去掉第一列
data = model_data.loc[ :,'X1':]
data.head()
```


X1|X2|X3|X4|X5|X6|X7|X8|X9
--|--|--|--|--|--|--|--|--
5458.2|13000|1376.2|2258.4|1315.9|529.0|2258.4|123.7|399.7
10550.0|11643|3502.5|3851.0|2288.7|1070.7|3181.9|211.1|610.2
6076.6|9047|1406.7|2092.6|1161.6|597.1|1968.3|45.9|302.3
2022.6|22068|822.8|960.0|703.7|361.9|941.4|115.7|171.8
10636.3|14397|3536.3|3967.2|2320.0|1141.3|3215.8|384.7|643.7


```python
## 2.查看相关系数矩阵，判定做变量降维的必要性（非必须）
corr_matrix = data.corr(method='pearson')
corr_matrix
```


X|X1|X2|X3|X4|X5|X6|X7|X8|X9
--|--|--|--|--|--|--|--|--|--
X1|1.000000|-0.094292|0.966506|0.979238|0.922984|0.921680|0.941148|0.637458|0.825568
X2|-0.094292|1.000000|0.112726|0.074167|0.214052|0.093483|-0.042776|0.081195|0.273145
X3|0.966506|0.112726|1.000000|0.985373|0.963159|0.939194|0.935196|0.704714|0.898016
X4|0.979238|0.074167|0.985373|1.000000|0.972862|0.939720|0.962267|0.713890|0.913364
X5|0.922984|0.214052|0.963159|0.972862|1.000000|0.971337|0.937109|0.716722|0.934549
X6|0.921680|0.093483|0.939194|0.939720|0.971337|1.000000|0.897127|0.624294|0.848004
X7|0.941148|-0.042776|0.935196|0.962267|0.937109|0.897127|1.000000|0.836272|0.928692
X8|0.637458|0.081195|0.704714|0.713890|0.716722|0.624294|0.836272|1.000000|0.881528
X9|0.825568|0.273145|0.898016|0.913364|0.934549|0.848004|0.928692|0.881528|1.000000


```python
## 3.做主成分之前，进行中心标准化
from sklearn import preprocessing
data = preprocessing.scale(data)
```


```python
## 4.使用sklearn的主成分分析，用于判断保留主成分的数量

#$ 说明：1、第一次的n_components参数应该设的大一点
## 说明：2、观察explained_variance_ratio_和explained_variance_的取值变化，建议explained_variance_ratio_累积大于0.85,explained_variance_需要保留的最后一个主成分大于0.8，



from sklearn.decomposition import PCA
pca = PCA(n_components=9)
pca.fit(data)
print(pca.explained_variance_)#解释的方差（对于标准化数据来说，单个主成分解释的方差应该大于1）

"""
[8.01129553e+00 1.22149318e+00 6.07923991e-01 9.88894782e-02
 3.86004507e-02 1.51746715e-02 4.88067443e-03 1.33610801e-03
 4.05920593e-04]
"""

print(pca.explained_variance_ratio_)

"""
[8.01129553e-01 1.22149318e-01 6.07923991e-02 9.88894782e-03
 3.86004507e-03 1.51746715e-03 4.88067443e-04 1.33610801e-04
 4.05920593e-05]
"""

```


```python
## 选取主成分保留个数
pca = PCA(n_components=2).fit(data)
#特征值
pca.explained_variance_
"""
array([8.01129553, 1.22149318])
"""
```


```python
#特征向量
#第一个主成分在第2个变量权重低,其余均高 
#第二个主成分在第2个变量权重高,其余均低
pd.DataFrame(pca.components_).T
```


X|0|1
--|--|--
0|0.353682|-0.212192
1|0.040555|0.942778
2|0.364148|-0.009845
3|0.367584|-0.045377
4|0.365917|0.095213
5|0.352119|-0.023027
6|0.364419|-0.135241
7|0.297565|0.048047
8|0.355405|0.183830


```python
## 计算主成分得分
#得分
newdata = pca.fit_transform(data)
newdata

"""
array([[-1.18945132, -0.31092235],
       [ 2.06415695, -0.74854414],
       [-1.43769023, -0.80669682],
       [-3.23039706,  0.84519783],
       [ 2.36892693, -0.44480961],
       [ 0.28997221,  2.79266758],
       [ 1.2099519 , -0.00638496],
       [-2.09689459, -0.22796377],
       [ 5.50091159, -0.14275827],
       [-3.47948639, -0.94978548]])
"""

```

```python
Cities_10_pca=model_data.join(pd.DataFrame(newdata))
Cities_10_pca = Cities_10_pca.rename(columns={0:'score1',1:'score2'})
Cities_10_pca
```

X|AREA|X1|X2|X3|X4|X5|X6|X7|X8|X9|score1|score2
--|--|--|--|--|--|--|--|--|--|--|--|--
0|辽宁|5458.2|13000|1376.2|2258.4|1315.9|529.0|2258.4|123.7|399.7|-1.189451|-0.310922
1|山东|10550.0|11643|3502.5|3851.0|2288.7|1070.7|3181.9|211.1|610.2|2.064157|-0.748544
2|河北|6076.6|9047|1406.7|2092.6|1161.6|597.1|1968.3|45.9|302.3|-1.437690|-0.806697
3|天津|2022.6|22068|822.8|960.0|703.7|361.9|941.4|115.7|171.8|-3.230397|0.845198
4|江苏|10636.3|14397|3536.3|3967.2|2320.0|1141.3|3215.8|384.7|643.7|2.368927|-0.444810
5|上海|5408.8|40627|2196.2|2755.8|1970.2|779.3|2035.2|320.5|709.0|0.289972|2.792668
6|浙江|7670.0|16570|2356.5|3065.0|2296.6|1180.6|2877.5|294.2|566.9|1.209952|-0.006385
7|福建|4682.0|13510|1047.1|1859.0|964.5|397.9|1663.3|173.7|272.9|-2.096895|-0.227964
8|广东|11769.7|15030|4224.6|4793.6|3022.9|1275.5|5013.6|1843.7|1201.6|5.500912|-0.142758
9|广西|2455.4|5062|367.0|995.7|542.2|352.7|1025.5|15.1|186.7|-3.479486|-0.949785







## 因子分析
主成分在每个原始变量上的权重分布不均匀, 有的权重很高, 有的权重很低, 这样主成分就具有了业务可解释性. 根据这个思路, 统计学家发明了因子旋转法, 尽量加大主成分在原始变量上权重的差异性, 是的原本主成分权重小的因子权重更小, 原本主成分权重大的因子权重更大, 最终提高了主成分的可解释性.

因子旋转法是在主成分分析的基础上进行`因子旋转`, 使得`主成分更容易解释`(称为因子).

假设随机变量的集合 $X=(X_1,X_2,...,X_p)$ ,因子分析模型可以被写为:

$$
X_1 = \mu_{1} + \alpha_{11}F_1 + \alpha_{12}F_2 + ... + \alpha_{1m}F_m + \epsilon_{1} \\
X_i = \mu_{i} + \alpha_{i1}F_1 + \alpha_{i2}F_2 + ... + \alpha_{im}F_m + \epsilon_{i} \\
X_p = \mu_{p} + \alpha_{p1}F_1 + \alpha_{p2}F_2 + ... + \alpha_{pm}F_m + \epsilon_{p}
$$

$\alpha_{im}$ 代表变量 $X_i$ 的第 $m$ 个公共因子 $F_m$ 的因子的系数, 又被称为`因子载荷`; $\epsilon_{i}$ 表示公共因子外的`随机因子`, 和公共因子两两正交; $\mu_{i}$ 表示 $X_i$ 的均值, 因子载荷矩阵为:

$$
\begin{pmatrix}
a_{11} & a_{12} & ... & a_{1m} \\
a_{21} & a_{22} & ... & a_{2m} \\
...    & ...    & ... & ... \\
a_{p1} & a_{p2} & ... & a_{pm} \\
\end{pmatrix}
$$

### 因子分析的重要概念
1. 因子载荷: 载荷 $a_{im}$ 表示第 $i$ 个变量与第 $m$ 个公共因子的相关系数, 表示 $X_i$ 依赖 $F_m$ 的分量(统计学称权重, 心理学称为载荷), $a_{im}$ 的绝对值越大, 表示相应的公共因子 $F_m$ 能提供的表达变量信息越多, 即认为信息更多`承载`在该因子上面. 可以使用`最大方差法`对`因子进行旋转`, 最大方差法的思想是使`因子载荷的平方和最大`, 即使`因子贡献的方差最大`, 使得各个因子的载荷`尽量拉开差距`, 从而得到便于解释因子的目的.
2. 变量共同度: 指`一个原始变量`所有因子上的`因子载荷平方和`, 代表所有因子合起来对原始变量的变异解释量. 共同度高表示某个原始变量与其他原始变量相关性高, 而共同度低则表明该原始变量与其他原始变量相关性很低.
3. 方差贡献: 公共因子 $F_m$ 的方差贡献就是在`所有变量中`该公共因子的`因子载荷平方和`, 用其可以衡量公共因子 $F_m$ 能够提供多少信息.


### 代码

```python
import pandas as pd
#1.导入数据
model_data = pd.read_csv("cities_10.csv",encoding='gbk')
model_data.head()
```

AREA|X1|X2|X3|X4|X5|X6|X7|X8|X9
--|--|--|--|--|--|--|--|--|--
辽宁|5458.2|13000|1376.2|2258.4|1315.9|529.0|2258.4|123.7|399.7
山东|10550.0|11643|3502.5|3851.0|2288.7|1070.7|3181.9|211.1|610.2
河北|6076.6|9047|1406.7|2092.6|1161.6|597.1|1968.3|45.9|302.3
天津|2022.6|22068|822.8|960.0|703.7|361.9|941.4|115.7|171.8
江苏|10636.3|14397|3536.3|3967.2|2320.0|1141.3|3215.8|384.7|643.7

字段含义:
X1|GDP
X2|人均GDP
X3|工业增加值
X4|第三产业增加值
X5|固定资产投资
X6|基本建设投资
X7|社会消费品零售总额
X8|海关出口总额
X9|地方财政收入


```python
## 去掉第一列
data = model_data.loc[ :,'X1':]
data.head()
```


X1|X2|X3|X4|X5|X6|X7|X8|X9
--|--|--|--|--|--|--|--|--
5458.2|13000|1376.2|2258.4|1315.9|529.0|2258.4|123.7|399.7
10550.0|11643|3502.5|3851.0|2288.7|1070.7|3181.9|211.1|610.2
6076.6|9047|1406.7|2092.6|1161.6|597.1|1968.3|45.9|302.3
2022.6|22068|822.8|960.0|703.7|361.9|941.4|115.7|171.8
10636.3|14397|3536.3|3967.2|2320.0|1141.3|3215.8|384.7|643.7



```python
## 3.做主成分之前，进行中心标准化
from sklearn import preprocessing
data = preprocessing.scale(data)
```


```python
## 4.使用sklearn的主成分分析，用于判断保留主成分的数量

#$ 说明：1、第一次的n_components参数应该设的大一点
## 说明：2、观察explained_variance_ratio_和explained_variance_的取值变化，建议explained_variance_ratio_累积大于0.85,explained_variance_需要保留的最后一个主成分大于0.8，



from sklearn.decomposition import PCA
pca = PCA(n_components=9)
pca.fit(data)
print(pca.explained_variance_)#解释的方差（对于标准化数据来说，单个主成分解释的方差应该大于1）

"""
[8.01129553e+00 1.22149318e+00 6.07923991e-01 9.88894782e-02
 3.86004507e-02 1.51746715e-02 4.88067443e-03 1.33610801e-03
 4.05920593e-04]
"""

print(pca.explained_variance_ratio_)

"""
[8.01129553e-01 1.22149318e-01 6.07923991e-02 9.88894782e-03
 3.86004507e-03 1.51746715e-03 4.88067443e-04 1.33610801e-04
 4.05920593e-05]
"""

```

使用fa_kit包提供的因子旋转法进行因子分析. 因子分析有四个步骤:
1. 数据导入与转换(load_data_samples)
2. 抽取主成分(extract_components)
3. 确定保留因子的个数(find_comps_to_retain)
4. 进行因子旋转(rotate_components)

```python
## 1.因子分析的概念很多，作为刚入门的人，我们可以认为因子分析是主成分分析的延续
from fa_kit import FactorAnalysis
from fa_kit import plotting as fa_plotting
fa = FactorAnalysis.load_data_samples(
        data,     #标准化以后的数据
        preproc_demean=True,
        preproc_scale=True
        ) 
fa.extract_components()  
## 2.设定提取主成分的方式。默认为“broken_stick”方法，建议使用“top_n”法
fa.find_comps_to_retain(method='top_n',num_keep=2)
## 3.通过最大方差法进行因子旋转
fa.rotate_components(method='varimax')
pd.DataFrame(fa.comps["rot"])#查看因子权重
```

X|0|1
--|--|--
0|0.362880|-0.196047
1|-0.001947|0.943648
2|0.364222|0.006565
3|0.369255|-0.028775
4|0.361258|0.111596
5|0.352799|-0.007144
6|0.370140|-0.118691
7|0.295099|0.061400
8|0.346765|0.199650

`comps`变量中保存因子旋转前后的数据, `rot`表示选取因子旋转后的数据.
在因子载荷矩阵中, 列为每个因子在原始变量上的因子载荷. 从结果上看, 除第二个变量外, 其他变量的载荷均较大;而第二个因子载荷上, 只有第二个变量的载荷相对大(忽略正负号). 分析原始变量发现, 第二个变量是人均GDP, 代表经济发展的人均规模, 其他变量均代表经济发展的总规模, 因此可以给每个因子命名, 如, 第一个因子可被命名为`经济发展总水平`, 第二个因子可被命名为`经济发展人均水平`



```python
## 4.获取因子得分
#到目前还没有与PCA中fit_transform类似的函数，因此只能手工计算因子
#以下是矩阵相乘的方式计算因子：因子=原始数据（n*k）*权重矩阵(k*num_keep)  

import numpy as np
fas = pd.DataFrame(fa.comps["rot"])   #因子权重
data = pd.DataFrame(data)#注意data数据需要标准化
fa_score = pd.DataFrame(np.dot(data, fas))
#因子得分
fa_score

```

X|0|1
--|--|--
0|-1.174241|-0.364178
1|2.095775|-0.654819
2|-1.399899|-0.870629
3|-3.265185|0.698849
4|2.386557|-0.337666
5|0.163901|2.802894
6|1.209012|0.048116
7|-2.084500|-0.322173
8|5.501759|0.105138
9|-3.433179|-1.105531

因子得分不能忽略正负号, 这与因子载荷矩阵及主成分向量不同.



### 代码二

```python
cities = pd.read_csv("cities_10.csv", encoding='gbk')
cities.ix[:, 'X1':].corr(method='pearson').head()

## 归一化
from sklearn.preprocessing import scale

scale_cities = scale(cities.ix[:,'X1':])
pca_c = PCA(n_components=3, whiten=True).fit(scale_cities)
pca_c.explained_variance_ratio_
pca_c1 = PCA(n_components=2, whiten=True).fit(scale_cities)
pd.DataFrame(pca_c1.components_)



## 因子分析
from sklearn.decomposition import FactorAnalysis

fa = FactorAnalysis(n_components=2).fit(scale_cities)
pd.DataFrame(fa.components_)
cities_scores = pd.DataFrame(fa.transform(scale_cities), 
                             columns=['total', 'avg'])
cities[['AREA']].join(cities_scores)
```





## 多维尺度分析
了解不同观测之间（不同产品， 不同服务， 不同品牌）的差异度和相似度， 用以发现产品之间的关系. 差异性和相似性一般用不同观测间的距离来衡量.

多维尺度分析(Multi-Dimensional Scaling, MDS)的技术可以将多个观测变量, 在较低的维度展示出来. 如果变量为连续变量则相应的多维尺度分析也被称为计量多维尺度(Metric MDS, MMDS). 基于等级变量的多维尺度分析被称为非计量多维尺度(Nonmetric MDS, NMMDS), 如, 用户对不同品牌的相似度评分属于定序分类变量.

多维尺度分析会在原始距离矩阵的基础上, 寻找每个观测值在较低维度上的位置, 以使得基于这个位置计算的距离矩阵与原始距离矩阵的差异尽可能少. 如, 城市间的距离是用球面上的相对位置计算出来的, 可以将原始位置看成是三维的, 经过多维尺度分析后, 可以将每个点的位置坐标降到二维, 因此多维尺度分析实际也起到了降维作用.



```python
## 使用美国各大城市距离数据CITY_DISTANCE
df = pd.read_csv('CITY_DISTANCE.csv', skipinitialspace=True)
df

```

X|City|Atlanta|Chicago|Denver|Houston|LosAngeles|Miami|NewYork|SanFrancisco|Seattle|Wanshington
--|--|--|--|--|--|--|--|--|--|--|--
0|Atlanta|0|NaN|NaN|NaN|NaN|NaN|NaN|NaN|NaN|NaN
1|Chicago|587|0.0|NaN|NaN|NaN|NaN|NaN|NaN|NaN|NaN
2|Denver|1212|920.0|0.0|NaN|NaN|NaN|NaN|NaN|NaN|NaN
3|Houston|701|940.0|879.0|0.0|NaN|NaN|NaN|NaN|NaN|NaN
4|LosAngeles|1936|1745.0|831.0|1374.0|0.0|NaN|NaN|NaN|NaN|NaN
5|Miami|604|1188.0|1726.0|968.0|2339.0|0.0|NaN|NaN|NaN|NaN
6|NewYork|748|713.0|1631.0|1420.0|2451.0|1092.0|0.0|NaN|NaN|NaN
7|SanFrancisco|2139|1858.0|949.0|1645.0|347.0|2594.0|2571.0|0.0|NaN|NaN
8|Seattle|2182|1737.0|1021.0|1891.0|959.0|2734.0|2408.0|678.0|0.0|NaN
9|Wanshington|543|597.0|1494.0|1220.0|2300.0|923.0|205.0|2442.0|2329.0|0.0


```python
df_filled = df.fillna(0)
distance_array = np.array(df_filled.ix[:, 1:])
cities = distance_array + distance_array.T
## 建模
from sklearn.manifold import MDS

mds = MDS(n_components=2, dissimilarity='precomputed', random_state=123)
mds.fit_transform(cities)
mds.stress_  # 350.0770309073701
mds.embedding_
"""
array([[  716.61625448,  -148.16931503],
       [  384.93295013,   338.19627187],
       [ -481.42120431,    25.85317473],
       [  157.13523411,  -574.28615625],
       [-1207.2958236 ,  -371.75217682],
       [ 1127.90413892,  -587.76572233],
       [ 1077.08560846,   506.5484897 ],
       [-1420.83429788,   -95.07851906],
       [-1336.36870742,   581.2975099 ],
       [  982.24584712,   325.1564433 ]])
"""

## 绘制感知地图
x = mds.embedding_[:, 0]
y = mds.embedding_[:, 1]
plt.scatter(x, y)
for a, b, s in zip(x, y, df['City']):
    plt.text(a, b, s, fontsize=12)
plt.show()

```